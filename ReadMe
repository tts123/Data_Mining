import pandas as pd
df = pd.read_csv('FIFA19.csv')
df.head()
Unnamed: 0	ID	Name	Age	Photo	Nationality	Flag	Overall	Potential	Club	...	Skill Moves	SlidingTackle	SprintSpeed	Stamina	StandingTackle	Strength	Vision	Volleys	Weak Foot	Work Rate
0	0	158023	L. Messi	31	https://cdn.sofifa.org/players/4/19/158023.png	Argentina	https://cdn.sofifa.org/flags/52.png	94	94	FC Barcelona	...	4	26	86	72	28	59	94	86	4	Medium/ Medium
1	1	20801	Cristiano Ronaldo	33	https://cdn.sofifa.org/players/4/19/20801.png	Portugal	https://cdn.sofifa.org/flags/38.png	94	94	Juventus	...	5	23	91	88	31	79	82	87	4	High/ Low
2	2	190871	Neymar Jr	26	https://cdn.sofifa.org/players/4/19/190871.png	Brazil	https://cdn.sofifa.org/flags/54.png	92	93	Paris Saint-Germain	...	5	33	90	81	24	49	87	84	5	High/ Medium
3	3	193080	De Gea	27	https://cdn.sofifa.org/players/4/19/193080.png	Spain	https://cdn.sofifa.org/flags/45.png	91	93	Manchester United	...	1	13	58	43	21	64	68	13	3	Medium/ Medium
4	4	192985	K. De Bruyne	27	https://cdn.sofifa.org/players/4/19/192985.png	Belgium	https://cdn.sofifa.org/flags/7.png	91	92	Manchester City	...	4	51	76	90	58	75	94	82	5	High/ High
5 rows × 87 columns

df.describe()
Unnamed: 0	ID	Age	Overall	Potential	Value (M)	Wage (K)	Special	Acceleration	Aggression	...	ShotPower	Skill Moves	SlidingTackle	SprintSpeed	Stamina	StandingTackle	Strength	Vision	Volleys	Weak Foot
count	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000	13526.000000	13533.000000	13725.000000	13725.000000	13725.000000	...	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000	13725.000000
mean	6862.000000	207876.744991	26.358251	69.243716	72.709727	283.479373	12.826424	1673.153515	66.000364	59.172969	...	58.900838	2.492678	47.973260	66.207723	66.204226	50.326412	67.646703	56.332022	45.914536	3.009107
std	3962.210557	30019.777724	4.279283	5.077007	5.734442	324.632817	25.080195	244.766572	14.468533	16.832899	...	16.730235	0.770074	21.614295	14.170072	15.052470	21.804304	11.881728	13.833376	17.828850	0.672234
min	0.000000	16.000000	16.000000	62.000000	62.000000	1.000000	1.000000	848.000000	12.000000	11.000000	...	2.000000	1.000000	4.000000	13.000000	12.000000	2.000000	24.000000	10.000000	4.000000	1.000000
25%	3431.000000	193030.000000	23.000000	65.000000	68.000000	3.000000	2.000000	1571.000000	58.000000	48.000000	...	51.000000	2.000000	26.000000	59.000000	61.000000	30.000000	61.000000	48.000000	33.000000	3.000000
50%	6862.000000	212518.000000	26.000000	68.000000	72.000000	24.500000	5.000000	1709.000000	68.000000	63.000000	...	63.000000	3.000000	57.000000	69.000000	69.000000	60.000000	69.000000	59.000000	48.000000	3.000000
75%	10293.000000	229641.000000	29.000000	72.000000	76.000000	575.000000	13.000000	1830.000000	76.000000	72.000000	...	71.000000	3.000000	66.000000	76.000000	76.000000	68.000000	76.000000	66.000000	60.000000	3.000000
max	13724.000000	246203.000000	45.000000	94.000000	95.000000	975.000000	565.000000	2346.000000	97.000000	95.000000	...	95.000000	5.000000	91.000000	96.000000	96.000000	93.000000	97.000000	94.000000	90.000000	5.000000
8 rows × 47 columns

df.shape
(13725, 87)
df.isnull().sum(axis = 0)
Unnamed: 0                 0
ID                         0
Name                       0
Age                        0
Photo                      0
Nationality                0
Flag                       0
Overall                    0
Potential                  0
Club                     192
Club Logo                  0
Value (M)                199
Wage (K)                 192
Special                    0
Acceleration               0
Aggression                 0
Agility                    0
Balance                    0
BallControl                0
Body Type                  0
CAM                     1302
CB                      1302
CDM                     1302
CF                      1302
CM                      1302
Composure                  0
Contract Valid Until    1302
Crossing                   0
Curve                      0
Dribbling                  0
                        ... 
Penalties                  0
Position                   5
Positioning                0
Preferred Foot             0
RAM                     1302
RB                      1302
RCB                     1302
RCM                     1302
RDM                     1302
RF                      1302
RM                      1302
RS                      1302
RW                      1302
RWB                     1302
Reactions                  0
Real Face                  0
Release Clause (M)         0
ST                      1302
ShortPassing               0
ShotPower                  0
Skill Moves                0
SlidingTackle              0
SprintSpeed                0
Stamina                    0
StandingTackle             0
Strength                   0
Vision                     0
Volleys                    0
Weak Foot                  0
Work Rate                  0
Length: 87, dtype: int64
df['Value (M)'].fillna((df['Value (M)'].mean()), inplace=True)
df['Wage (K)'].fillna((df['Wage (K)'].mean()), inplace=True)
df.loc[df['Value (M)'] < 100, 'V'] = 0
df.loc[df['Value (M)'] >= 100, 'V'] = 1
df_lc = df.copy()
df_lc['V'].isnull().values.any()
False
df.head()
Unnamed: 0	ID	Name	Age	Photo	Nationality	Flag	Overall	Potential	Club	...	SlidingTackle	SprintSpeed	Stamina	StandingTackle	Strength	Vision	Volleys	Weak Foot	Work Rate	V
0	0	158023	L. Messi	31	https://cdn.sofifa.org/players/4/19/158023.png	Argentina	https://cdn.sofifa.org/flags/52.png	94	94	FC Barcelona	...	26	86	72	28	59	94	86	4	Medium/ Medium	1.0
1	1	20801	Cristiano Ronaldo	33	https://cdn.sofifa.org/players/4/19/20801.png	Portugal	https://cdn.sofifa.org/flags/38.png	94	94	Juventus	...	23	91	88	31	79	82	87	4	High/ Low	0.0
2	2	190871	Neymar Jr	26	https://cdn.sofifa.org/players/4/19/190871.png	Brazil	https://cdn.sofifa.org/flags/54.png	92	93	Paris Saint-Germain	...	33	90	81	24	49	87	84	5	High/ Medium	1.0
3	3	193080	De Gea	27	https://cdn.sofifa.org/players/4/19/193080.png	Spain	https://cdn.sofifa.org/flags/45.png	91	93	Manchester United	...	13	58	43	21	64	68	13	3	Medium/ Medium	0.0
4	4	192985	K. De Bruyne	27	https://cdn.sofifa.org/players/4/19/192985.png	Belgium	https://cdn.sofifa.org/flags/7.png	91	92	Manchester City	...	51	76	90	58	75	94	82	5	High/ High	1.0
5 rows × 88 columns

df.dtypes
Unnamed: 0                int64
ID                        int64
Name                     object
Age                       int64
Photo                    object
Nationality              object
Flag                     object
Overall                   int64
Potential                 int64
Club                     object
Club Logo                object
Value (M)               float64
Wage (K)                float64
Special                   int64
Acceleration              int64
Aggression                int64
Agility                   int64
Balance                   int64
BallControl               int64
Body Type                object
CAM                      object
CB                       object
CDM                      object
CF                       object
CM                       object
Composure                 int64
Contract Valid Until     object
Crossing                  int64
Curve                     int64
Dribbling                 int64
                         ...   
Position                 object
Positioning               int64
Preferred Foot           object
RAM                      object
RB                       object
RCB                      object
RCM                      object
RDM                      object
RF                       object
RM                       object
RS                       object
RW                       object
RWB                      object
Reactions                 int64
Real Face                object
Release Clause (M)      float64
ST                       object
ShortPassing              int64
ShotPower                 int64
Skill Moves               int64
SlidingTackle             int64
SprintSpeed               int64
Stamina                   int64
StandingTackle            int64
Strength                  int64
Vision                    int64
Volleys                   int64
Weak Foot                 int64
Work Rate                object
V                       float64
Length: 88, dtype: object
%matplotlib inline
import random
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import math
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
df['Age'].isnull().values.any()
False
df['Nationality'].isnull().values.any()
False
df['Overall'].isnull().values.any()
False
target = df['V']
df['Value (M)'].fillna((df['Value (M)'].mean()), inplace=True)
df['Wage (K)'].fillna((df['Wage (K)'].mean()), inplace=True)
df['Value (M)'].isnull().values.any()
False
df_lc['V'].isnull().values.any()
False
df['Wage (K)'].isnull().values.any()
False
df['Work Rate'].unique()
array(['Medium/ Medium', 'High/ Low', 'High/ Medium', 'High/ High',
       'Medium/ High', 'Medium/ Low', 'Low/ High', 'Low/ Medium',
       'Low/ Low'], dtype=object)
#df_lc = df.copy()
df_lc['Nationality'] = df_lc['Nationality'].astype('category')
df_lc['Club'] = df_lc['Club'].astype('category') 
df_lc['Work Rate'] = df_lc['Work Rate'].astype('category')
#df_lc['CREATE_DT_TM'] = df_lc['CREATE_DT_TM'].astype('category') 
#df_lc['SCHED_START_DT_TM'] = df_lc['SCHED_START_DT_TM'].astype('category')
#df_lc['RACE'] = df_lc['RACE'].astype('category')
#df_lc['ETHNIC_GROUP'] = df_lc['ETHNIC_GROUP'].astype('category')

df_lc['Nationality'] = df_lc['Nationality'].cat.codes
df_lc['Club'] = df_lc['Club'].cat.codes 
df_lc['Work Rate'] = df_lc['Work Rate'].cat.codes
#df_lc['CREATE_DT_TM'] = df_lc['CREATE_DT_TM'].cat.codes 
#df_lc['SCHED_START_DT_TM'] = df_lc['SCHED_START_DT_TM'].cat.codes
#df_lc['RACE'] = df_lc['RACE'].cat.codes

#df_lc['ETHNIC_GROUP'] = df_lc['ETHNIC_GROUP'].cat.codes

print(df_lc.dtypes)
Unnamed: 0                int64
ID                        int64
Name                     object
Age                       int64
Photo                    object
Nationality               int16
Flag                     object
Overall                   int64
Potential                 int64
Club                      int16
Club Logo                object
Value (M)               float64
Wage (K)                float64
Special                   int64
Acceleration              int64
Aggression                int64
Agility                   int64
Balance                   int64
BallControl               int64
Body Type                object
CAM                      object
CB                       object
CDM                      object
CF                       object
CM                       object
Composure                 int64
Contract Valid Until     object
Crossing                  int64
Curve                     int64
Dribbling                 int64
                         ...   
Position                 object
Positioning               int64
Preferred Foot           object
RAM                      object
RB                       object
RCB                      object
RCM                      object
RDM                      object
RF                       object
RM                       object
RS                       object
RW                       object
RWB                      object
Reactions                 int64
Real Face                object
Release Clause (M)      float64
ST                       object
ShortPassing              int64
ShotPower                 int64
Skill Moves               int64
SlidingTackle             int64
SprintSpeed               int64
Stamina                   int64
StandingTackle            int64
Strength                  int64
Vision                    int64
Volleys                   int64
Weak Foot                 int64
Work Rate                  int8
V                       float64
Length: 88, dtype: object
df_lc
Unnamed: 0	ID	Name	Age	Photo	Nationality	Flag	Overall	Potential	Club	...	SlidingTackle	SprintSpeed	Stamina	StandingTackle	Strength	Vision	Volleys	Weak Foot	Work Rate	V
0	0	158023	L. Messi	31	https://cdn.sofifa.org/players/4/19/158023.png	5	https://cdn.sofifa.org/flags/52.png	94	94	211	...	26	86	72	28	59	94	86	4	8	1.0
1	1	20801	Cristiano Ronaldo	33	https://cdn.sofifa.org/players/4/19/20801.png	118	https://cdn.sofifa.org/flags/38.png	94	94	325	...	23	91	88	31	79	82	87	4	1	0.0
2	2	190871	Neymar Jr	26	https://cdn.sofifa.org/players/4/19/190871.png	17	https://cdn.sofifa.org/flags/54.png	92	93	432	...	33	90	81	24	49	87	84	5	2	1.0
3	3	193080	De Gea	27	https://cdn.sofifa.org/players/4/19/193080.png	133	https://cdn.sofifa.org/flags/45.png	91	93	372	...	13	58	43	21	64	68	13	3	8	0.0
4	4	192985	K. De Bruyne	27	https://cdn.sofifa.org/players/4/19/192985.png	12	https://cdn.sofifa.org/flags/7.png	91	92	371	...	51	76	90	58	75	94	82	5	0	1.0
5	5	183277	E. Hazard	27	https://cdn.sofifa.org/players/4/19/183277.png	12	https://cdn.sofifa.org/flags/7.png	91	91	133	...	22	89	83	27	66	89	80	4	2	0.0
6	6	177003	L. Modriƒá	32	https://cdn.sofifa.org/players/4/19/177003.png	32	https://cdn.sofifa.org/flags/10.png	91	91	467	...	73	72	89	76	58	92	76	4	0	0.0
7	7	176580	L. Su√°rez	31	https://cdn.sofifa.org/players/4/19/176580.png	152	https://cdn.sofifa.org/flags/60.png	91	91	211	...	38	75	90	45	83	84	88	4	2	0.0
8	8	155862	Sergio Ramos	32	https://cdn.sofifa.org/players/4/19/155862.png	133	https://cdn.sofifa.org/flags/45.png	91	91	467	...	91	75	84	92	83	63	66	3	2	0.0
9	9	200389	J. Oblak	25	https://cdn.sofifa.org/players/4/19/200389.png	131	https://cdn.sofifa.org/flags/44.png	90	93	61	...	18	60	41	12	78	70	13	3	8	0.0
10	10	192119	T. Courtois	26	https://cdn.sofifa.org/players/4/19/192119.png	12	https://cdn.sofifa.org/flags/7.png	90	91	467	...	16	52	38	18	70	44	12	3	8	0.0
11	11	188545	R. Lewandowski	29	https://cdn.sofifa.org/players/4/19/188545.png	117	https://cdn.sofifa.org/flags/37.png	90	90	213	...	19	78	78	42	84	77	89	4	2	0.0
12	12	182521	T. Kroos	28	https://cdn.sofifa.org/players/4/19/182521.png	56	https://cdn.sofifa.org/flags/21.png	90	90	467	...	69	62	75	79	73	86	82	5	8	0.0
13	13	182493	D. God√≠n	32	https://cdn.sofifa.org/players/4/19/182493.png	152	https://cdn.sofifa.org/flags/60.png	90	90	61	...	89	68	66	89	88	52	47	3	6	0.0
14	14	167495	M. Neuer	32	https://cdn.sofifa.org/players/4/19/167495.png	56	https://cdn.sofifa.org/flags/21.png	90	90	213	...	11	60	43	10	80	70	11	4	8	0.0
15	15	215914	N. Kant√©	27	https://cdn.sofifa.org/players/4/19/215914.png	52	https://cdn.sofifa.org/flags/18.png	89	90	133	...	85	78	96	91	76	79	56	3	6	0.0
16	16	211110	P. Dybala	24	https://cdn.sofifa.org/players/4/19/211110.png	5	https://cdn.sofifa.org/flags/52.png	89	94	325	...	20	83	80	20	65	87	88	3	2	0.0
17	17	202126	H. Kane	24	https://cdn.sofifa.org/players/4/19/202126.png	43	https://cdn.sofifa.org/flags/14.png	89	92	580	...	38	72	89	36	84	80	82	4	0	0.0
18	18	194765	A. Griezmann	27	https://cdn.sofifa.org/players/4/19/194765.png	52	https://cdn.sofifa.org/flags/18.png	89	90	61	...	48	85	83	47	62	83	87	3	0	0.0
19	19	192448	M. ter Stegen	26	https://cdn.sofifa.org/players/4/19/192448.png	56	https://cdn.sofifa.org/flags/21.png	89	92	211	...	10	50	35	13	79	69	14	4	8	0.0
20	20	189511	Sergio Busquets	29	https://cdn.sofifa.org/players/4/19/189511.png	133	https://cdn.sofifa.org/flags/45.png	89	89	211	...	80	52	86	86	77	87	44	3	8	0.0
21	21	179813	E. Cavani	31	https://cdn.sofifa.org/players/4/19/179813.png	152	https://cdn.sofifa.org/flags/60.png	89	89	432	...	39	76	92	45	78	77	90	3	0	0.0
22	22	178603	M. Hummels	29	https://cdn.sofifa.org/players/4/19/178603.png	56	https://cdn.sofifa.org/flags/21.png	89	89	213	...	90	68	66	92	85	79	60	3	2	0.0
23	23	168542	David Silva	32	https://cdn.sofifa.org/players/4/19/168542.png	133	https://cdn.sofifa.org/flags/45.png	89	89	371	...	29	64	78	51	52	92	80	2	2	0.0
24	24	153079	S. Ag√ºero	30	https://cdn.sofifa.org/players/4/19/153079.png	5	https://cdn.sofifa.org/flags/52.png	89	89	371	...	12	80	76	20	73	83	85	4	2	0.0
25	25	138956	G. Chiellini	33	https://cdn.sofifa.org/players/4/19/138956.png	74	https://cdn.sofifa.org/flags/27.png	89	89	325	...	90	72	65	93	89	50	45	2	6	0.0
26	26	209331	M. Salah	26	https://cdn.sofifa.org/players/4/19/209331.png	41	https://cdn.sofifa.org/flags/111.png	88	89	361	...	41	91	84	43	70	82	73	3	2	0.0
27	27	200145	Casemiro	26	https://cdn.sofifa.org/players/4/19/200145.png	17	https://cdn.sofifa.org/flags/54.png	88	91	467	...	87	65	87	90	89	77	53	3	6	0.0
28	28	198710	J. Rodr√≠guez	26	https://cdn.sofifa.org/players/4/19/198710.png	28	https://cdn.sofifa.org/flags/56.png	88	89	213	...	44	69	70	41	68	89	90	3	8	0.0
29	29	198219	L. Insigne	27	https://cdn.sofifa.org/players/4/19/198219.png	74	https://cdn.sofifa.org/flags/27.png	88	88	395	...	22	86	75	24	44	87	74	3	2	0.0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
13695	13695	243841	S. Pittman	25	https://cdn.sofifa.org/players/4/19/243841.png	126	https://cdn.sofifa.org/flags/42.png	62	65	362	...	53	76	77	56	59	54	53	3	2	1.0
13696	13696	181377	E. Bajrami	30	https://cdn.sofifa.org/players/4/19/181377.png	138	https://cdn.sofifa.org/flags/46.png	62	62	305	...	21	63	51	31	58	60	58	2	7	1.0
13697	13697	140418	R. Duffy	32	https://cdn.sofifa.org/players/4/19/140418.png	155	https://cdn.sofifa.org/flags/50.png	62	62	407	...	54	41	76	60	70	42	17	3	0	1.0
13698	13698	214658	J. Karlstr√∂m	23	https://cdn.sofifa.org/players/4/19/214658.png	138	https://cdn.sofifa.org/flags/46.png	62	68	190	...	52	60	73	57	71	62	58	3	2	1.0
13699	13699	221826	Li Tixiang	28	https://cdn.sofifa.org/players/4/19/221826.png	27	https://cdn.sofifa.org/flags/155.png	62	62	280	...	57	69	63	60	73	37	31	3	8	1.0
13700	13700	239746	L. Watkowiak	22	https://cdn.sofifa.org/players/4/19/239746.png	56	https://cdn.sofifa.org/flags/21.png	62	68	507	...	13	23	26	13	83	32	5	3	8	1.0
13701	13701	240258	O. Sangante	27	https://cdn.sofifa.org/players/4/19/240258.png	127	https://cdn.sofifa.org/flags/136.png	62	63	348	...	54	53	78	64	76	58	34	3	8	1.0
13702	13702	236675	K. Jakob	20	https://cdn.sofifa.org/players/4/19/236675.png	56	https://cdn.sofifa.org/flags/21.png	62	76	210	...	58	67	67	56	59	41	35	3	8	1.0
13703	13703	245635	Dani Sandoval	20	https://cdn.sofifa.org/players/4/19/245635.png	133	https://cdn.sofifa.org/flags/45.png	62	80	468	...	22	73	52	15	43	56	58	3	7	1.0
13704	13704	229508	S. Sprangler	23	https://cdn.sofifa.org/players/4/19/229508.png	8	https://cdn.sofifa.org/flags/4.png	62	68	636	...	57	68	72	60	67	63	42	3	8	1.0
13705	13705	231556	T. Schmidt	24	https://cdn.sofifa.org/players/4/19/231556.png	56	https://cdn.sofifa.org/flags/21.png	62	64	612	...	25	68	71	19	46	53	47	4	8	1.0
13706	13706	242308	I. Chenihi	28	https://cdn.sofifa.org/players/4/19/242308.png	2	https://cdn.sofifa.org/flags/97.png	62	62	31	...	40	74	58	40	56	58	38	3	2	1.0
13707	13707	216453	L. Shephard	23	https://cdn.sofifa.org/players/4/19/216453.png	155	https://cdn.sofifa.org/flags/50.png	62	66	255	...	62	71	71	62	59	47	22	2	2	1.0
13708	13708	234629	K. Dirks Riis	21	https://cdn.sofifa.org/players/4/19/234629.png	38	https://cdn.sofifa.org/flags/13.png	62	74	202	...	58	70	72	60	75	33	35	3	2	1.0
13709	13709	235397	N. Freeman	22	https://cdn.sofifa.org/players/4/19/235397.png	43	https://cdn.sofifa.org/flags/14.png	62	68	638	...	35	77	72	35	59	58	43	3	8	1.0
13710	13710	201094	B. Garratt	24	https://cdn.sofifa.org/players/4/19/201094.png	43	https://cdn.sofifa.org/flags/14.png	62	69	164	...	11	48	30	15	49	49	15	2	8	1.0
13711	13711	213894	H. White	23	https://cdn.sofifa.org/players/4/19/213894.png	43	https://cdn.sofifa.org/flags/14.png	62	63	373	...	58	92	73	62	67	32	38	3	2	1.0
13712	13712	225670	J. Baird	22	https://cdn.sofifa.org/players/4/19/225670.png	126	https://cdn.sofifa.org/flags/42.png	62	71	554	...	56	73	66	62	70	36	22	3	8	1.0
13713	13713	226182	Sui Weijie	35	https://cdn.sofifa.org/players/4/19/226182.png	27	https://cdn.sofifa.org/flags/155.png	62	62	137	...	17	58	28	13	62	57	15	3	8	0.0
13714	13714	230534	S. Mvibudulu	25	https://cdn.sofifa.org/players/4/19/230534.png	37	https://cdn.sofifa.org/flags/110.png	62	64	495	...	32	87	72	28	71	38	52	3	7	1.0
13715	13715	238214	M. Degerlund	20	https://cdn.sofifa.org/players/4/19/238214.png	138	https://cdn.sofifa.org/flags/46.png	62	69	306	...	62	66	67	65	75	43	29	3	8	1.0
13716	13716	225927	Q. Lecoeuche	24	https://cdn.sofifa.org/players/4/19/225927.png	52	https://cdn.sofifa.org/flags/18.png	62	68	590	...	61	73	72	62	50	53	27	3	8	1.0
13717	13717	235655	M. Al Enazi	34	https://cdn.sofifa.org/players/4/19/235655.png	125	https://cdn.sofifa.org/flags/183.png	62	62	29	...	57	71	64	60	36	54	35	4	8	0.0
13718	13718	245639	Andr√© Anderson	18	https://cdn.sofifa.org/players/4/19/245639.png	17	https://cdn.sofifa.org/flags/54.png	62	79	591	...	15	72	65	20	47	65	57	3	8	1.0
13719	13719	208776	F. Stritzel	24	https://cdn.sofifa.org/players/4/19/208776.png	56	https://cdn.sofifa.org/flags/21.png	62	67	503	...	12	36	27	13	71	24	8	3	8	1.0
13720	13720	235656	W. Al Anazi	23	https://cdn.sofifa.org/players/4/19/235656.png	125	https://cdn.sofifa.org/flags/183.png	62	69	39	...	56	66	77	57	60	60	37	3	2	1.0
13721	13721	208009	W. Jobello	24	https://cdn.sofifa.org/players/4/19/208009.png	52	https://cdn.sofifa.org/flags/18.png	62	67	261	...	19	70	60	17	57	55	64	3	8	1.0
13722	13722	209546	Rafhael Lucas	25	https://cdn.sofifa.org/players/4/19/209546.png	17	https://cdn.sofifa.org/flags/54.png	62	67	319	...	43	83	54	45	56	53	59	3	8	1.0
13723	13723	226186	Luo Xin	28	https://cdn.sofifa.org/players/4/19/226186.png	27	https://cdn.sofifa.org/flags/155.png	62	63	73	...	61	63	68	64	66	60	44	2	3	1.0
13724	13724	236938	M. Wilks	19	https://cdn.sofifa.org/players/4/19/236938.png	43	https://cdn.sofifa.org/flags/14.png	62	76	191	...	15	79	69	16	64	49	48	4	2	1.0
13725 rows × 88 columns

df_lc[['Age','Nationality','Overall','Potential','Club','SlidingTackle','SprintSpeed','Stamina'
              ,'StandingTackle','Strength','Vision','Volleys','Weak Foot','Work Rate','Agility','BallControl'
              ]]=df_lc[['Age','Nationality','Overall','Potential','Club','SlidingTackle','SprintSpeed','Stamina'
              ,'StandingTackle','Strength','Vision','Volleys','Weak Foot','Work Rate','Agility','BallControl'
              ]].astype(float)
df_lc[['Age','Nationality','Overall','Potential','Club','SlidingTackle','SprintSpeed','Stamina'
               ,'StandingTackle','Strength','Vision','Volleys','Weak Foot','Work Rate','Agility','BallControl'
              ]].dtypes
Age               float64
Nationality       float64
Overall           float64
Potential         float64
Club              float64
SlidingTackle     float64
SprintSpeed       float64
Stamina           float64
StandingTackle    float64
Strength          float64
Vision            float64
Volleys           float64
Weak Foot         float64
Work Rate         float64
Agility           float64
BallControl       float64
dtype: object
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
mms.fit(df_lc[['Age','Nationality','Overall','Potential','Club','SlidingTackle','SprintSpeed','Stamina'
               ,'StandingTackle','Strength','Vision','Volleys','Weak Foot','Work Rate','Agility','BallControl'
              ]])
data_transformed = mms.transform(df_lc[['Age','Nationality','Overall','Potential','Club','SlidingTackle','SprintSpeed','Stamina'
                                        ,'StandingTackle','Strength','Vision','Volleys','Weak Foot','Work Rate','Agility','BallControl'
                                       ]])
from sklearn import metrics
from sklearn.cluster import KMeans
Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(data_transformed)
    Sum_of_squared_distances.append(km.inertia_)
    
plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

from sklearn.cluster import KMeans
clusterer = KMeans(6, random_state=1)
clusterer.fit(data_transformed)
KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',
       random_state=1, tol=0.0001, verbose=0)
df_lc['clust_grp'] = clusterer.predict(data_transformed)
sns.lmplot(data = df_lc, x = 'Overall', y = 'V', hue = 'clust_grp', fit_reg = False)
<seaborn.axisgrid.FacetGrid at 0x21d2276f710>

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df_lc[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl'
                                                          ]], 
                                                    df_lc['V'],
                                                    test_size=0.2, 
                                                    random_state=1)

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 1)
import xgboost as xgb
dtrain = xgb.DMatrix(data = x_train, label = y_train)
dval = xgb.DMatrix(data = x_val, label = y_val)
dtest = xgb.DMatrix(data = x_test, label = y_test)
C:\Users\sherp\Anaconda3\lib\site-packages\xgboost\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version
  if getattr(data, 'base', None) is not None and \
param = {'max_depth':3,
         'eta': 0.35,
         'silent':1,
         'objective':'binary:logistic',
         'eval_metric': 'logloss'
         #,'gamma': ???,
         #,'lambda': ???,
         #,'alpha': ???,
         #,'min_child_weight': ???,
         #,'colsample_bytree' :??? 
         #,colsample_bynode' : ???
         #,'scale_pos_weight' : ???
         ,'maximize' : 'FALSE'
         ,'n_jobs' : -1
         #,'base_score' : ???
         #,'max_delta_step' : ???
        }
watchlist = [(dtrain, 'train'), (dval, 'eval')]
num_round = 25
bst = xgb.train(param, dtrain, num_round, watchlist, early_stopping_rounds = 10)
[0]	train-logloss:0.470543	eval-logloss:0.470578
Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.

Will train until eval-logloss hasn't improved in 10 rounds.
[1]	train-logloss:0.350043	eval-logloss:0.3496
[2]	train-logloss:0.274883	eval-logloss:0.276003
[3]	train-logloss:0.223641	eval-logloss:0.225261
[4]	train-logloss:0.186119	eval-logloss:0.189057
[5]	train-logloss:0.159369	eval-logloss:0.163812
[6]	train-logloss:0.140694	eval-logloss:0.143478
[7]	train-logloss:0.128948	eval-logloss:0.132057
[8]	train-logloss:0.119647	eval-logloss:0.122731
[9]	train-logloss:0.110769	eval-logloss:0.113955
[10]	train-logloss:0.101956	eval-logloss:0.105805
[11]	train-logloss:0.0948	eval-logloss:0.097881
[12]	train-logloss:0.088759	eval-logloss:0.091449
[13]	train-logloss:0.085338	eval-logloss:0.088145
[14]	train-logloss:0.080684	eval-logloss:0.083602
[15]	train-logloss:0.078603	eval-logloss:0.081434
[16]	train-logloss:0.075	eval-logloss:0.077557
[17]	train-logloss:0.073255	eval-logloss:0.075273
[18]	train-logloss:0.070988	eval-logloss:0.07365
[19]	train-logloss:0.06849	eval-logloss:0.071119
[20]	train-logloss:0.066649	eval-logloss:0.069192
[21]	train-logloss:0.064243	eval-logloss:0.067035
[22]	train-logloss:0.063234	eval-logloss:0.065579
[23]	train-logloss:0.061395	eval-logloss:0.063834
[24]	train-logloss:0.059922	eval-logloss:0.062242
LOS_train_w_preds = x_train
LOS_train_w_preds['xgb_probs'] = bst.predict(dtrain)

LOS_test_w_preds = x_test
LOS_test_w_preds['xgb_probs'] = bst.predict(dtest)
from sklearn import metrics
y = y_test
scores = LOS_test_w_preds['xgb_probs']
fpr, tpr, thresholds = metrics.roc_curve(y, scores)
metrics.auc(fpr, tpr)
0.9972096049484817
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

plt.plot(roc_curve(y_train, LOS_train_w_preds['xgb_probs'])[0],roc_curve(y_train, LOS_train_w_preds['xgb_probs'])[1], 
         color = 'blue', label='Train ROC Curve (area = %0.2f)' % roc_auc_score(y_train, LOS_train_w_preds['xgb_probs']))

plt.plot(roc_curve(y_test, LOS_test_w_preds['xgb_probs'])[0],roc_curve(y_test, LOS_test_w_preds['xgb_probs'])[1], 
         color = 'red', label='Test ROC Curve (area = %0.2f)' % roc_auc_score(y_test, LOS_test_w_preds['xgb_probs']))


plt.plot([0, 1], [0, 1], color='black', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

param_auc = {'max_depth':3,
         'eta': 0.35,
         'silent':1,
         'objective':'binary:logistic',
         'eval_metric': 'auc'
         #,'gamma': ???,
         #,'lambda': ???,
         #,'alpha': ???,
         #,'min_child_weight': ???,
         #,'colsample_bytree' :??? 
         #,colsample_bynode' : ???
         #,'scale_pos_weight' : ???
         ,'maximize' : 'TRUE'
         ,'n_jobs' : -1
         #,'base_score' : ???
         #,'max_delta_step' : ???
        }
watchlist = [(dtrain, 'train'), (dval, 'eval')]
num_round = 25
bst_auc = xgb.train(param_auc, dtrain, num_round, watchlist, early_stopping_rounds = 10)
[0]	train-auc:0.962117	eval-auc:0.96169
Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.

Will train until eval-auc hasn't improved in 10 rounds.
[1]	train-auc:0.980078	eval-auc:0.981256
[2]	train-auc:0.984336	eval-auc:0.984726
[3]	train-auc:0.988053	eval-auc:0.989249
[4]	train-auc:0.992212	eval-auc:0.992007
[5]	train-auc:0.994082	eval-auc:0.99356
[6]	train-auc:0.994461	eval-auc:0.994133
[7]	train-auc:0.994758	eval-auc:0.994358
[8]	train-auc:0.99527	eval-auc:0.994779
[9]	train-auc:0.995527	eval-auc:0.99513
[10]	train-auc:0.996319	eval-auc:0.995639
[11]	train-auc:0.996651	eval-auc:0.996515
[12]	train-auc:0.99715	eval-auc:0.996989
[13]	train-auc:0.997154	eval-auc:0.99702
[14]	train-auc:0.997519	eval-auc:0.99737
[15]	train-auc:0.997613	eval-auc:0.99746
[16]	train-auc:0.997826	eval-auc:0.997763
[17]	train-auc:0.997866	eval-auc:0.997891
[18]	train-auc:0.997916	eval-auc:0.997901
[19]	train-auc:0.998114	eval-auc:0.998049
[20]	train-auc:0.998272	eval-auc:0.998208
[21]	train-auc:0.998416	eval-auc:0.998346
[22]	train-auc:0.998446	eval-auc:0.998407
[23]	train-auc:0.998526	eval-auc:0.998506
[24]	train-auc:0.998555	eval-auc:0.998555
LOS_train_w_preds['xgb_probs_auc'] = bst_auc.predict(dtrain)
LOS_test_w_preds['xgb_probs_auc'] = bst_auc.predict(dtest)
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

plt.plot(roc_curve(y_train, LOS_train_w_preds['xgb_probs_auc'])[0],roc_curve(y_train, LOS_train_w_preds['xgb_probs_auc'])[1], 
         color = 'blue', label='Train ROC Curve (area = %0.2f)' % roc_auc_score(y_train, LOS_train_w_preds['xgb_probs_auc']))

plt.plot(roc_curve(y_test, LOS_test_w_preds['xgb_probs_auc'])[0],roc_curve(y_test, LOS_test_w_preds['xgb_probs_auc'])[1], 
         color = 'red', label='Test ROC Curve (area = %0.2f)' % roc_auc_score(y_test, LOS_test_w_preds['xgb_probs_auc']))


plt.plot([0, 1], [0, 1], color='black', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#lightgbm
param_aucpr = {'max_depth':3,
         'eta': 0.01,
         'silent':1,
         'objective':'binary:logistic',
         'eval_metric': 'aucpr'
         #,'gamma': ???,
         #,'lambda': ???,
         #,'alpha': ???,
         #,'min_child_weight': ???,
         #,'colsample_bytree' :??? 
         #,colsample_bynode' : ???
         #,'scale_pos_weight' : ???
         ,'maximize' : 'TRUE'
         ,'n_jobs' : -1
         #,'base_score' : ???
         #,'max_delta_step' : ???
        }
watchlist = [(dtrain, 'train'), (dval, 'eval')]
bst_aucpr = xgb.train(param_aucpr, dtrain, num_round, watchlist, early_stopping_rounds = 10)
[0]	train-aucpr:0.947447	eval-aucpr:0.942643
Multiple eval metrics have been passed: 'eval-aucpr' will be used for early stopping.

Will train until eval-aucpr hasn't improved in 10 rounds.
[1]	train-aucpr:0.9724	eval-aucpr:0.971484
[2]	train-aucpr:0.972027	eval-aucpr:0.971373
[3]	train-aucpr:0.9724	eval-aucpr:0.971484
[4]	train-aucpr:0.972027	eval-aucpr:0.971373
[5]	train-aucpr:0.9724	eval-aucpr:0.971484
[6]	train-aucpr:0.972027	eval-aucpr:0.971373
[7]	train-aucpr:0.972603	eval-aucpr:0.971551
[8]	train-aucpr:0.972513	eval-aucpr:0.9716
[9]	train-aucpr:0.972889	eval-aucpr:0.971862
[10]	train-aucpr:0.973568	eval-aucpr:0.972635
[11]	train-aucpr:0.973832	eval-aucpr:0.972804
[12]	train-aucpr:0.973616	eval-aucpr:0.97264
[13]	train-aucpr:0.973755	eval-aucpr:0.972481
[14]	train-aucpr:0.973832	eval-aucpr:0.972804
[15]	train-aucpr:0.973755	eval-aucpr:0.972481
[16]	train-aucpr:0.973715	eval-aucpr:0.972445
[17]	train-aucpr:0.976358	eval-aucpr:0.975357
[18]	train-aucpr:0.976363	eval-aucpr:0.975264
[19]	train-aucpr:0.976394	eval-aucpr:0.975382
[20]	train-aucpr:0.976373	eval-aucpr:0.975372
[21]	train-aucpr:0.976801	eval-aucpr:0.975822
[22]	train-aucpr:0.976646	eval-aucpr:0.975867
[23]	train-aucpr:0.976953	eval-aucpr:0.976278
[24]	train-aucpr:0.976974	eval-aucpr:0.976226
LOS_train_w_preds['xgb_probs_aucpr'] = bst_aucpr.predict(dtrain)
LOS_test_w_preds['xgb_probs_aucpr'] = bst_aucpr.predict(dtest)
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

plt.plot(roc_curve(y_train, LOS_train_w_preds['xgb_probs_aucpr'])[0],roc_curve(y_train, LOS_train_w_preds['xgb_probs_aucpr'])[1], 
         color = 'blue', label='Train ROC Curve (area = %0.2f)' % roc_auc_score(y_train, LOS_train_w_preds['xgb_probs_aucpr']))

plt.plot(roc_curve(y_test, LOS_test_w_preds['xgb_probs_aucpr'])[0],roc_curve(y_test, LOS_test_w_preds['xgb_probs_aucpr'])[1], 
         color = 'red', label='Test ROC Curve (area = %0.2f)' % roc_auc_score(y_test, LOS_test_w_preds['xgb_probs_aucpr']))


plt.plot([0, 1], [0, 1], color='black', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

import lightgbm as lgb
lgb_LOS_train = lgb.Dataset(x_train.drop(columns = ['xgb_probs', 'xgb_probs_auc', 'xgb_probs_aucpr'])
                                          , y_train)
lgb_val_train = lgb.Dataset(x_val, y_val)
lgb_params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'max_depth' : 3,
    #'num_leaves' : ???
    'learning_rate': 0.1,
    #'num_threads' : -1,
    #'scale_pos_weight' : ???
    'early_stopping_round' : 10,
    # min_data_in_leaf = ???,
    # pos_bagging_fraction = ???,
    # neg_bagging_fraction = ???,
    # bagging_freq = ???,
    # max_delta_step = ???,
    #'top_rate' : ???
    #'other_rate' : ???
    #'lambda_l1' : ???
    #'lambda_l2' : ???
}
lgb_gbm = lgb.train(params = lgb_params, train_set = lgb_LOS_train,
                num_boost_round = 100, valid_sets = [lgb_val_train, lgb_LOS_train],
               valid_names = ['Evaluation', 'Train'])
[1]	Train's binary_logloss: 0.619897	Evaluation's binary_logloss: 0.619586
Training until validation scores don't improve for 10 rounds
[2]	Train's binary_logloss: 0.55825	Evaluation's binary_logloss: 0.558681
[3]	Train's binary_logloss: 0.506822	Evaluation's binary_logloss: 0.506745
[4]	Train's binary_logloss: 0.463462	Evaluation's binary_logloss: 0.463561
[5]	Train's binary_logloss: 0.424571	Evaluation's binary_logloss: 0.425011
[6]	Train's binary_logloss: 0.39243	Evaluation's binary_logloss: 0.392827
[7]	Train's binary_logloss: 0.363934	Evaluation's binary_logloss: 0.364032
[8]	Train's binary_logloss: 0.33829	Evaluation's binary_logloss: 0.338898
[9]	Train's binary_logloss: 0.316111	Evaluation's binary_logloss: 0.316496
[10]	Train's binary_logloss: 0.295015	Evaluation's binary_logloss: 0.295898
[11]	Train's binary_logloss: 0.276773	Evaluation's binary_logloss: 0.278135
[12]	Train's binary_logloss: 0.259863	Evaluation's binary_logloss: 0.261334
[13]	Train's binary_logloss: 0.244332	Evaluation's binary_logloss: 0.246062
[14]	Train's binary_logloss: 0.231379	Evaluation's binary_logloss: 0.233416
[15]	Train's binary_logloss: 0.219666	Evaluation's binary_logloss: 0.221163
[16]	Train's binary_logloss: 0.208365	Evaluation's binary_logloss: 0.210199
[17]	Train's binary_logloss: 0.197844	Evaluation's binary_logloss: 0.19936
[18]	Train's binary_logloss: 0.188597	Evaluation's binary_logloss: 0.190166
[19]	Train's binary_logloss: 0.18038	Evaluation's binary_logloss: 0.181713
[20]	Train's binary_logloss: 0.173005	Evaluation's binary_logloss: 0.174128
[21]	Train's binary_logloss: 0.166305	Evaluation's binary_logloss: 0.167936
[22]	Train's binary_logloss: 0.160173	Evaluation's binary_logloss: 0.162111
[23]	Train's binary_logloss: 0.154505	Evaluation's binary_logloss: 0.156124
[24]	Train's binary_logloss: 0.149164	Evaluation's binary_logloss: 0.151009
[25]	Train's binary_logloss: 0.143488	Evaluation's binary_logloss: 0.145763
[26]	Train's binary_logloss: 0.139103	Evaluation's binary_logloss: 0.141635
[27]	Train's binary_logloss: 0.134435	Evaluation's binary_logloss: 0.137353
[28]	Train's binary_logloss: 0.130746	Evaluation's binary_logloss: 0.133428
[29]	Train's binary_logloss: 0.12698	Evaluation's binary_logloss: 0.130153
[30]	Train's binary_logloss: 0.123794	Evaluation's binary_logloss: 0.12716
[31]	Train's binary_logloss: 0.120645	Evaluation's binary_logloss: 0.124209
[32]	Train's binary_logloss: 0.11836	Evaluation's binary_logloss: 0.121803
[33]	Train's binary_logloss: 0.115675	Evaluation's binary_logloss: 0.119099
[34]	Train's binary_logloss: 0.113255	Evaluation's binary_logloss: 0.1166
[35]	Train's binary_logloss: 0.110973	Evaluation's binary_logloss: 0.114138
[36]	Train's binary_logloss: 0.108506	Evaluation's binary_logloss: 0.111661
[37]	Train's binary_logloss: 0.106311	Evaluation's binary_logloss: 0.109325
[38]	Train's binary_logloss: 0.10331	Evaluation's binary_logloss: 0.106106
[39]	Train's binary_logloss: 0.101302	Evaluation's binary_logloss: 0.104349
[40]	Train's binary_logloss: 0.0997122	Evaluation's binary_logloss: 0.103104
[41]	Train's binary_logloss: 0.0979558	Evaluation's binary_logloss: 0.101189
[42]	Train's binary_logloss: 0.0964328	Evaluation's binary_logloss: 0.0994991
[43]	Train's binary_logloss: 0.0950947	Evaluation's binary_logloss: 0.0984509
[44]	Train's binary_logloss: 0.0927675	Evaluation's binary_logloss: 0.0959384
[45]	Train's binary_logloss: 0.0912691	Evaluation's binary_logloss: 0.0943191
[46]	Train's binary_logloss: 0.0898468	Evaluation's binary_logloss: 0.0930199
[47]	Train's binary_logloss: 0.0883308	Evaluation's binary_logloss: 0.0913808
[48]	Train's binary_logloss: 0.087495	Evaluation's binary_logloss: 0.0906374
[49]	Train's binary_logloss: 0.0863207	Evaluation's binary_logloss: 0.0895292
[50]	Train's binary_logloss: 0.0854794	Evaluation's binary_logloss: 0.0885869
[51]	Train's binary_logloss: 0.0843943	Evaluation's binary_logloss: 0.0877204
[52]	Train's binary_logloss: 0.0833609	Evaluation's binary_logloss: 0.0866355
[53]	Train's binary_logloss: 0.0823627	Evaluation's binary_logloss: 0.0855539
[54]	Train's binary_logloss: 0.0816138	Evaluation's binary_logloss: 0.0848526
[55]	Train's binary_logloss: 0.0800812	Evaluation's binary_logloss: 0.0832768
[56]	Train's binary_logloss: 0.0792448	Evaluation's binary_logloss: 0.0824506
[57]	Train's binary_logloss: 0.0787247	Evaluation's binary_logloss: 0.081867
[58]	Train's binary_logloss: 0.0778772	Evaluation's binary_logloss: 0.0812304
[59]	Train's binary_logloss: 0.0773558	Evaluation's binary_logloss: 0.0807799
[60]	Train's binary_logloss: 0.0761167	Evaluation's binary_logloss: 0.07966
[61]	Train's binary_logloss: 0.0753005	Evaluation's binary_logloss: 0.0791959
[62]	Train's binary_logloss: 0.074308	Evaluation's binary_logloss: 0.0783282
[63]	Train's binary_logloss: 0.0738132	Evaluation's binary_logloss: 0.077783
[64]	Train's binary_logloss: 0.073395	Evaluation's binary_logloss: 0.0773601
[65]	Train's binary_logloss: 0.0723499	Evaluation's binary_logloss: 0.0763154
[66]	Train's binary_logloss: 0.0718841	Evaluation's binary_logloss: 0.075902
[67]	Train's binary_logloss: 0.0710344	Evaluation's binary_logloss: 0.0749087
[68]	Train's binary_logloss: 0.0700977	Evaluation's binary_logloss: 0.0742182
[69]	Train's binary_logloss: 0.069675	Evaluation's binary_logloss: 0.0737606
[70]	Train's binary_logloss: 0.0693526	Evaluation's binary_logloss: 0.0736158
[71]	Train's binary_logloss: 0.0689998	Evaluation's binary_logloss: 0.0733239
[72]	Train's binary_logloss: 0.0682554	Evaluation's binary_logloss: 0.0725471
[73]	Train's binary_logloss: 0.0677137	Evaluation's binary_logloss: 0.0719708
[74]	Train's binary_logloss: 0.0673614	Evaluation's binary_logloss: 0.0717004
[75]	Train's binary_logloss: 0.0670998	Evaluation's binary_logloss: 0.0715511
[76]	Train's binary_logloss: 0.0663244	Evaluation's binary_logloss: 0.0710504
[77]	Train's binary_logloss: 0.0656251	Evaluation's binary_logloss: 0.0702262
[78]	Train's binary_logloss: 0.0650211	Evaluation's binary_logloss: 0.0695133
[79]	Train's binary_logloss: 0.0646707	Evaluation's binary_logloss: 0.069247
[80]	Train's binary_logloss: 0.0643756	Evaluation's binary_logloss: 0.0689657
[81]	Train's binary_logloss: 0.0635971	Evaluation's binary_logloss: 0.0681478
[82]	Train's binary_logloss: 0.0628579	Evaluation's binary_logloss: 0.0676374
[83]	Train's binary_logloss: 0.0620579	Evaluation's binary_logloss: 0.0669068
[84]	Train's binary_logloss: 0.0618302	Evaluation's binary_logloss: 0.0667786
[85]	Train's binary_logloss: 0.0615748	Evaluation's binary_logloss: 0.0665671
[86]	Train's binary_logloss: 0.0612052	Evaluation's binary_logloss: 0.066333
[87]	Train's binary_logloss: 0.0609711	Evaluation's binary_logloss: 0.0659995
[88]	Train's binary_logloss: 0.0607149	Evaluation's binary_logloss: 0.0656575
[89]	Train's binary_logloss: 0.0600218	Evaluation's binary_logloss: 0.0651724
[90]	Train's binary_logloss: 0.0594285	Evaluation's binary_logloss: 0.0645344
[91]	Train's binary_logloss: 0.0590144	Evaluation's binary_logloss: 0.0642306
[92]	Train's binary_logloss: 0.0587849	Evaluation's binary_logloss: 0.0639685
[93]	Train's binary_logloss: 0.0583666	Evaluation's binary_logloss: 0.0636549
[94]	Train's binary_logloss: 0.0581954	Evaluation's binary_logloss: 0.0634346
[95]	Train's binary_logloss: 0.0579927	Evaluation's binary_logloss: 0.0632522
[96]	Train's binary_logloss: 0.0576153	Evaluation's binary_logloss: 0.0628671
[97]	Train's binary_logloss: 0.0572885	Evaluation's binary_logloss: 0.0628123
[98]	Train's binary_logloss: 0.0571179	Evaluation's binary_logloss: 0.0626889
[99]	Train's binary_logloss: 0.0566287	Evaluation's binary_logloss: 0.0622092
[100]	Train's binary_logloss: 0.0561325	Evaluation's binary_logloss: 0.0618831
Did not meet early stopping. Best iteration is:
[100]	Train's binary_logloss: 0.0561325	Evaluation's binary_logloss: 0.0618831
C:\Users\sherp\Anaconda3\lib\site-packages\lightgbm\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument
  warnings.warn("Found `{}` in params. Will use it instead of argument".format(alias))
y_probs_train = lgb_gbm.predict(x_train.drop(columns = ['xgb_probs', 'xgb_probs_auc', 'xgb_probs_aucpr']))
y_probs_test = lgb_gbm.predict(x_test)
fpr, tpr, thresholds = metrics.roc_curve(y_train, y_probs_train)
metrics.auc(fpr, tpr)
0.9990847942366046
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_probs_test)
metrics.auc(fpr, tpr)
0.99785544831761
difference=0.788120704959381-0.7835160911854211
difference
0.004604613773959931
#catboost
x_train_cat = x_train.drop(columns = ['xgb_probs', 'xgb_probs_auc', 'xgb_probs_aucpr'])
x_val_cat = x_val
x_test_cat = x_test.drop(columns = ['xgb_probs', 'xgb_probs_auc', 'xgb_probs_aucpr'])
x_train_cat[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl'
                                                          ]] = x_train_cat[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl']].astype(str)

x_val_cat[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl']] = x_val_cat[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl']].astype(str)

x_test_cat[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl']] = x_test_cat[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl']].astype(str)
x_train_cat.head()
Age	Nationality	Overall	Potential	Club	SlidingTackle	SprintSpeed	Stamina	StandingTackle	Strength	Vision	Volleys	Weak Foot	Work Rate	Agility	BallControl
995	27.0	56.0	77.0	78.0	86.0	78.0	64.0	69.0	78.0	74.0	71.0	55.0	3.0	0.0	70.0	73.0
1046	30.0	152.0	77.0	77.0	267.0	80.0	80.0	83.0	77.0	73.0	52.0	59.0	3.0	2.0	79.0	69.0
10736	24.0	43.0	65.0	68.0	316.0	21.0	90.0	62.0	25.0	58.0	51.0	54.0	3.0	2.0	73.0	60.0
11670	19.0	12.0	64.0	77.0	333.0	36.0	77.0	41.0	35.0	55.0	60.0	51.0	4.0	8.0	71.0	67.0
8039	30.0	74.0	67.0	67.0	235.0	12.0	68.0	56.0	13.0	74.0	50.0	68.0	4.0	7.0	67.0	65.0
import numpy as np
predictors = x_train_cat
categorical_var = np.where(predictors.dtypes != np.float)[0]
print('\nCategorical Variables indices : ',categorical_var)
Categorical Variables indices :  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
from catboost import CatBoostClassifier, Pool, cv
cat_boost_model = CatBoostClassifier(
    loss_function = 'Logloss',
    random_seed=42,
    iterations = 10,
    learning_rate = 0.03,
    early_stopping_rounds = 10,
    #l2_leaf_reg = ???
    depth = 3
    
)
cat_boost_model.fit(
    x_train_cat, y_train
    ,cat_features=categorical_var,
    eval_set=(x_val_cat, y_val)
    , plot = True
)
0:	learn: 0.6730365	test: 0.6728081	best: 0.6728081 (0)	total: 157ms	remaining: 1.41s
1:	learn: 0.6540668	test: 0.6535395	best: 0.6535395 (1)	total: 159ms	remaining: 638ms
2:	learn: 0.6355522	test: 0.6346202	best: 0.6346202 (2)	total: 166ms	remaining: 388ms
3:	learn: 0.6182280	test: 0.6169777	best: 0.6169777 (3)	total: 169ms	remaining: 253ms
4:	learn: 0.6014465	test: 0.5999719	best: 0.5999719 (4)	total: 171ms	remaining: 171ms
5:	learn: 0.5858534	test: 0.5838196	best: 0.5838196 (5)	total: 173ms	remaining: 116ms
6:	learn: 0.5707123	test: 0.5684647	best: 0.5684647 (6)	total: 176ms	remaining: 75.3ms
7:	learn: 0.5567095	test: 0.5540783	best: 0.5540783 (7)	total: 178ms	remaining: 44.5ms
8:	learn: 0.5434796	test: 0.5409525	best: 0.5409525 (8)	total: 184ms	remaining: 20.4ms
9:	learn: 0.5306074	test: 0.5277187	best: 0.5277187 (9)	total: 186ms	remaining: 0us

bestTest = 0.5277186624
bestIteration = 9

<catboost.core.CatBoostClassifier at 0x21d28acf828>
catboost_probs_train = cat_boost_model.predict_proba(x_train_cat)
catboost_probs = cat_boost_model.predict_proba(x_test_cat)
catboost_probs_df_train = pd.DataFrame(catboost_probs_train)
catboost_probs_df_train = catboost_probs_df_train.add_prefix('cat')

catboost_probs_df = pd.DataFrame(catboost_probs)
catboost_probs_df = catboost_probs_df.add_prefix('cat')
fprc, tprc, thresholds = metrics.roc_curve(y_train, catboost_probs_df_train['cat1'])
metrics.auc(fprc, tprc)
0.9665776697468413
fprc, tprc, thresholds = metrics.roc_curve(y_test, catboost_probs_df['cat1'])
metrics.auc(fprc, tprc)
0.9639281557867991
difference=0.8354900825617795-0.8174167022637224
difference
0.01807338029805705
catboost_probs.shape
(2745, 2)
catboost_probs_train
array([[0.62116029, 0.37883971],
       [0.62116029, 0.37883971],
       [0.37563388, 0.62436612],
       ...,
       [0.61641858, 0.38358142],
       [0.62116029, 0.37883971],
       [0.37563388, 0.62436612]])
catboost_probs_train.shape
(8784, 2)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df_lc[['Age','Nationality','Overall','Potential','Club',
                                                            'SlidingTackle','SprintSpeed','Stamina', 
                                                            'StandingTackle','Strength','Vision','Volleys',
                                                            'Weak Foot','Work Rate','Agility','BallControl'
                                                          ]], 
                                                    df_lc['V'],
                                                    test_size=0.2, 
                                                    random_state=1)

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 1)
from sklearn.ensemble import RandomForestClassifier
rf_cv = RandomForestClassifier()
rfc_param_grid = {
    'bootstrap': [True],
    'max_depth': [2, 3, 5#, 80, 90, 100, 110
                 ],
    'max_features': [2, 3, 5],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [10, 22],
    'criterion' : ['gini', 'entropy']
    #,min_impurity_decrease : [???]
    #,min_impurity_split : [???]
    #,class_weight : [???]
}
from sklearn.model_selection import GridSearchCV
rfc_gs = GridSearchCV(estimator=rf_cv, param_grid=rfc_param_grid, cv= 5, scoring = 'roc_auc'
                     #scoring methods also include: accuracy, balanced accuracy, average precision, f1, etc.  see documentation
                     )
dfs = [x_train, x_val]
dfsy = [y_train, y_val]
x_train_rf = pd.concat(dfs)
y_train_rf = pd.concat(dfsy)
import hyperopt as hp
from hyperopt import Trials,fmin,STATUS_OK
import lightgbm as lgb

lightgbm_hp_train = lgb.Dataset(x_train_rf, y_train_rf)
lightgbm_hp_train = lgb.Dataset(x_train, y_train)
lightgbm_hp_val = lgb.Dataset(x_val, y_val)
import numpy as np
lgbm_space = {

    #'boosting_type': hp.hp.choice('boosting_type',  ['gbdt']),
    'num_leaves':hp.hp.choice('num_leaves', np.arange(10, 300,1, dtype=int)),

    'subsample':hp.hp.quniform('subsample',0.5,1.0,0.05),
    'colsample_bytree':hp.hp.quniform('colsample_bytree',0.5,1.0,0.05),
    'min_child_weight':hp.hp.quniform('min_child_weight', 100, 1000,100),
    'reg_alpha': hp.hp.uniform('reg_alpha', 0.0, 1000.0),
    'reg_lambda': hp.hp.uniform('reg_lambda', 0.0, 1000.0),
    'learning_rate': hp.hp.loguniform('learning_rate', -4, 0),
    'feature_fraction': hp.hp.loguniform('feature_fraction', -4, 0),
    'bagging_fraction': hp.hp.loguniform('bagging_fraction', -4, 0),
    'bagging_frequency':hp.hp.choice('bagging_frequency', np.arange(5, 100,1, dtype=int)),
    'drop_rate': hp.hp.loguniform('drop_rate', -4, 0),
    'scale_pos_weight': hp.hp.uniform('scale_pos_weight', 6.0, 10.0),
    
    'num_class' : 1, 
    'metric' : 'auc',
    'nthread': 6, 
    'max_bin': 512
    }
def objective_m(params, n_folds=5):


    model = lgb.cv(params = params,
              train_set = lightgbm_hp_train,
              num_boost_round = 10000,
              early_stopping_rounds = 10,
             nfold = n_folds)
  
    
  
    # returns the best average loss on validation set 
    
    loss = 1 - (max(model['auc-mean']))
    return loss


bayes_trials = Trials()
MAX_EVALS = 100 # this controls the runtime 

lgbm_best_m = fmin(fn = objective_m, space = lgbm_space, algo = hp.tpe.suggest, 
max_evals = MAX_EVALS, trials = bayes_trials)
100%|███████████████████████████████████████████████| 100/100 [00:48<00:00,  2.57it/s, best loss: 0.006999490528427188]
lgbm_best_m
{'bagging_fraction': 0.1267294821905753,
 'bagging_frequency': 74,
 'colsample_bytree': 1.0,
 'drop_rate': 0.040770283498754625,
 'feature_fraction': 0.15941915445524668,
 'learning_rate': 0.10511338507441492,
 'min_child_weight': 100.0,
 'num_leaves': 243,
 'reg_alpha': 2.5694396821882424,
 'reg_lambda': 914.6372419969246,
 'scale_pos_weight': 6.965950966169017,
 'subsample': 0.5}
lgb_gbm_auc = lgb.train(params = lgbm_best_m, train_set = lgb_LOS_train,
                num_boost_round = 100, valid_sets = [lgb_val_train, lgb_LOS_train],
               valid_names = ['Evaluation', 'Train'])
lgb_gbm_auc
<lightgbm.basic.Booster at 0x21d2b3f1e48>
watchlist = [(dtrain, 'train'), (dval, 'eval')]
num_round = 25
best_auc = xgb.train(lgbm_best_m, dtrain, num_round, watchlist, early_stopping_rounds = 10)
[0]	train-rmse:0.470795	eval-rmse:0.471325
Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.

Will train until eval-rmse hasn't improved in 10 rounds.
[1]	train-rmse:0.446999	eval-rmse:0.447856
[2]	train-rmse:0.424716	eval-rmse:0.425961
[3]	train-rmse:0.405138	eval-rmse:0.40697
[4]	train-rmse:0.389041	eval-rmse:0.391335
[5]	train-rmse:0.373981	eval-rmse:0.376647
[6]	train-rmse:0.361348	eval-rmse:0.364325
[7]	train-rmse:0.350327	eval-rmse:0.353658
[8]	train-rmse:0.340746	eval-rmse:0.344442
[9]	train-rmse:0.333386	eval-rmse:0.337571
[10]	train-rmse:0.325619	eval-rmse:0.330289
[11]	train-rmse:0.31964	eval-rmse:0.32454
[12]	train-rmse:0.314308	eval-rmse:0.319699
[13]	train-rmse:0.308808	eval-rmse:0.314539
[14]	train-rmse:0.302686	eval-rmse:0.309049
[15]	train-rmse:0.298496	eval-rmse:0.304934
[16]	train-rmse:0.294796	eval-rmse:0.301546
[17]	train-rmse:0.290252	eval-rmse:0.297311
[18]	train-rmse:0.287331	eval-rmse:0.29464
[19]	train-rmse:0.284071	eval-rmse:0.291781
[20]	train-rmse:0.282379	eval-rmse:0.290466
[21]	train-rmse:0.280761	eval-rmse:0.289369
[22]	train-rmse:0.278223	eval-rmse:0.287022
[23]	train-rmse:0.275617	eval-rmse:0.284751
[24]	train-rmse:0.272359	eval-rmse:0.281693
LOS_train_w_preds['xgb_probs_auc'] = best_auc.predict(dtrain)
LOS_test_w_preds['xgb_probs_auc'] = best_auc.predict(dtest)
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

plt.plot(roc_curve(y_train, LOS_train_w_preds['xgb_probs_auc'])[0],roc_curve(y_train, LOS_train_w_preds['xgb_probs_auc'])[1], 
         color = 'blue', label='Train ROC Curve (area = %0.3f)' % roc_auc_score(y_train, LOS_train_w_preds['xgb_probs_auc']))

plt.plot(roc_curve(y_test, LOS_test_w_preds['xgb_probs_auc'])[0],roc_curve(y_test, LOS_test_w_preds['xgb_probs_auc'])[1], 
         color = 'red', label='Test ROC Curve (area = %0.3f)' % roc_auc_score(y_test, LOS_test_w_preds['xgb_probs_auc']))


plt.plot([0, 1], [0, 1], color='black', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

y_probs_train = lgb_gbm_auc.predict(x_train)
y_probs_test = lgb_gbm_auc.predict(x_test)
fpr, tpr, thresholds = metrics.roc_curve(y_train, y_probs_train)
metrics.auc(fpr, tpr)
0.9806895680544082
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_probs_test)
metrics.auc(fpr, tpr)
0.9757334478657327
difference=0.8229433785376189-0.8055372048045544
difference
0.017406173733064523
import hyperopt
def hyperopt_objective(params):
    model1 = CatBoostClassifier(
        l2_leaf_reg=(params['l2_leaf_reg']),
        learning_rate=params['learning_rate'],
        iterations=500,
        eval_metric='Accuracy',
        random_seed=42,
        logging_level='Silent',
        loss_function='Logloss'
        
    )
    
    cv_data = cv(
        Pool(x_train_cat, y_train, cat_features=categorical_var),
        model1.get_params()
    )
    best_accuracy = np.max(cv_data['test-Accuracy-mean'])
    
    return 1 - best_accuracy # as hyperopt minimises

from numpy.random import RandomState

params_space = {
    'l2_leaf_reg': hyperopt.hp.qloguniform('l2_leaf_reg', 0, 2, 1),
    'learning_rate': hyperopt.hp.uniform('learning_rate', 1e-3, 5e-1),
    'depth': hyperopt.hp.choice('depth', [3,4,5,6,8]),
    'iterations': hyperopt.hp.choice('iterations', range(20,1020,20)), 
#'depth': hyperopt.hp.choice('depth', range(1, 11,1)),

}

trials = hyperopt.Trials()

best = hyperopt.fmin(
    hyperopt_objective,
    space=params_space,
    algo=hyperopt.tpe.suggest,
    max_evals=50,
    trials=trials
)

print(best)
from catboost import CatBoostClassifier, Pool, cv
cat_boost_model = CatBoostClassifier(
    loss_function = 'Logloss',
    random_seed=42,
    iterations = 29,
    learning_rate = 0.10534620463009686,
    early_stopping_rounds = 10,
    l2_leaf_reg = 3,
    depth = 3
    
)
cat_boost_model.fit(
    x_train_cat, y_train
    ,cat_features=categorical_var,
    eval_set=(x_val_cat, y_val)
    , plot = True
)
catboost_probs_train = cat_boost_model.predict_proba(x_train_cat)
catboost_probs = cat_boost_model.predict_proba(x_test_cat)
catboost_probs_df_train = pd.DataFrame(catboost_probs_train)
catboost_probs_df_train = catboost_probs_df_train.add_prefix('cat')

catboost_probs_df = pd.DataFrame(catboost_probs)
catboost_probs_df = catboost_probs_df.add_prefix('cat')
fprc, tprc, thresholds = metrics.roc_curve(y_train, catboost_probs_df_train['cat1'])
metrics.auc(fprc, tprc)
fprc, tprc, thresholds = metrics.roc_curve(y_test, catboost_probs_df['cat1'])
metrics.auc(fprc, tprc)
diff= 0.8374760134104592-0.8217871118622712
diff
1) I think my Catboost model was best so I decided to work with hyperopt to get better result. It definitely improve my auc values. I think the model is well fit since the difference is not that close and the train auc is neither too low or nor too high.

rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1)
rf_clf_model = rf_clf.fit(x_train, y_train)
C:\Users\sherp\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  "10 in version 0.20 to 100 in 0.22.", FutureWarning)
rf_train_probs = rf_clf_model.predict_proba(x_train)
rf_test_probs = rf_clf_model.predict_proba(x_test)
rf_train_prob_cols = ['V', 'proba']
rf_preds_train = pd.DataFrame(rf_train_probs, columns=rf_train_prob_cols)
rf_preds_test = pd.DataFrame(rf_test_probs, columns=rf_train_prob_cols)
from sklearn import metrics
fpr, tpr, thresholds = metrics.roc_curve(y_train, rf_preds_train['proba'])
metrics.auc(fpr, tpr)
0.9999954107479143
fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_preds_test['proba'])
metrics.auc(fpr, tpr)
0.9948995958683078
rf_clf.feature_importances_
array([0.08237673, 0.00791061, 0.30251477, 0.22617023, 0.03030636,
       0.02334843, 0.01535121, 0.01439263, 0.03736774, 0.01141963,
       0.0369194 , 0.01586687, 0.00255323, 0.00401265, 0.01123948,
       0.17825003])
rf_clf_feat_names = x_train.columns
rf_clf_importances = rf_clf.feature_importances_
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.pyplot import figure
figure(num=None, figsize = (20,20))

indices = np.argsort(rf_clf_importances)

plt.figure(1)
plt.title('LOS')
plt.barh(range(len(indices)), rf_clf_importances[indices], color = 'b', align = 'center')
plt.yticks(range(len(indices)), rf_clf_feat_names[indices])
plt.xlabel('Relative Importance: Gini')
Text(0.5,0,'Relative Importance: Gini')

rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1, criterion = 'gini', max_features = 5, max_depth = 13,
                               min_samples_split = 500,
                               min_samples_leaf = 30
                               #min_weight_fraction = ???,
                               #min_impurity_decrease = ???,
                               #class_weight = ???
                               )
rf_clf_model = rf_clf.fit(x_train, y_train)
C:\Users\sherp\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  "10 in version 0.20 to 100 in 0.22.", FutureWarning)
rf_train_probs = rf_clf_model.predict_proba(x_train)
rf_test_probs = rf_clf_model.predict_proba(x_test)
rf_train_prob_cols = ['V', 'proba']
rf_preds_train = pd.DataFrame(rf_train_probs, columns=rf_train_prob_cols)
rf_preds_test = pd.DataFrame(rf_test_probs, columns=rf_train_prob_cols)
fpr, tpr, thresholds = metrics.roc_curve(y_train, rf_preds_train['proba'])
metrics.auc(fpr, tpr)
0.9868307392564312
fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_preds_test['proba'])
metrics.auc(fpr, tpr)
0.9844518004748974
rf_clf_importances = rf_clf.feature_importances_
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.pyplot import figure
figure(num=None, figsize = (20,20))

indices = np.argsort(rf_clf_importances)

plt.figure(1)
plt.title('V')
plt.barh(range(len(indices)), rf_clf_importances[indices], color = 'b', align = 'center')
plt.yticks(range(len(indices)), rf_clf_feat_names[indices])
plt.xlabel('Relative Importance: Gini')
Text(0.5,0,'Relative Importance: Gini')

import lightgbm as gbm
from lightgbm import LGBMClassifier
gbm_clf = gbm.LGBMClassifier(boosting_type = 'gbdt',
                             #num_leaves = ,
                             #max_depth = ,
                             learning_rate = 0.1
                             #n_estimators = 
                             #,subsample_for_bin =
                             ,objective = 'binary'
                             ,metric = 'binary_logloss'
                         #,class_weight = 
                         #,min_split_gain =
                         #,min_split_weight =
                         #,min_child_weight =
                         #,min_child_samples =
                         #,subsample =
                         #,subsample_freq =
                         #,colsample_bytree =
                         ,reg_alpha = 5
                         ,reg_lambda = 120
                         ,importance_type = 'split' #will rank features by # of times it is used in model.'gain' for gain
                             ,num_iterations = 1000
                       )
gbm_clf.fit(x_train, y_train, eval_metric = 'logloss', verbose = True, eval_set = [(x_val, y_val)],
       early_stopping_rounds = 20)
C:\Users\sherp\Anaconda3\lib\site-packages\lightgbm\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn("Found `{}` in params. Will use it instead of argument".format(alias))
[1]	valid_0's binary_logloss: 0.629184
Training until validation scores don't improve for 20 rounds
[2]	valid_0's binary_logloss: 0.575068
[3]	valid_0's binary_logloss: 0.52837
[4]	valid_0's binary_logloss: 0.48845
[5]	valid_0's binary_logloss: 0.453913
[6]	valid_0's binary_logloss: 0.423151
[7]	valid_0's binary_logloss: 0.395747
[8]	valid_0's binary_logloss: 0.371717
[9]	valid_0's binary_logloss: 0.350554
[10]	valid_0's binary_logloss: 0.331114
[11]	valid_0's binary_logloss: 0.313586
[12]	valid_0's binary_logloss: 0.297535
[13]	valid_0's binary_logloss: 0.283412
[14]	valid_0's binary_logloss: 0.270661
[15]	valid_0's binary_logloss: 0.259437
[16]	valid_0's binary_logloss: 0.248897
[17]	valid_0's binary_logloss: 0.239269
[18]	valid_0's binary_logloss: 0.23015
[19]	valid_0's binary_logloss: 0.222017
[20]	valid_0's binary_logloss: 0.214409
[21]	valid_0's binary_logloss: 0.207333
[22]	valid_0's binary_logloss: 0.200612
[23]	valid_0's binary_logloss: 0.194424
[24]	valid_0's binary_logloss: 0.18854
[25]	valid_0's binary_logloss: 0.183144
[26]	valid_0's binary_logloss: 0.178385
[27]	valid_0's binary_logloss: 0.173924
[28]	valid_0's binary_logloss: 0.169683
[29]	valid_0's binary_logloss: 0.165709
[30]	valid_0's binary_logloss: 0.162263
[31]	valid_0's binary_logloss: 0.158637
[32]	valid_0's binary_logloss: 0.155534
[33]	valid_0's binary_logloss: 0.152527
[34]	valid_0's binary_logloss: 0.14968
[35]	valid_0's binary_logloss: 0.147087
[36]	valid_0's binary_logloss: 0.144532
[37]	valid_0's binary_logloss: 0.142181
[38]	valid_0's binary_logloss: 0.139827
[39]	valid_0's binary_logloss: 0.137974
[40]	valid_0's binary_logloss: 0.13589
[41]	valid_0's binary_logloss: 0.133905
[42]	valid_0's binary_logloss: 0.13205
[43]	valid_0's binary_logloss: 0.130418
[44]	valid_0's binary_logloss: 0.129017
[45]	valid_0's binary_logloss: 0.127374
[46]	valid_0's binary_logloss: 0.126034
[47]	valid_0's binary_logloss: 0.12475
[48]	valid_0's binary_logloss: 0.123321
[49]	valid_0's binary_logloss: 0.121867
[50]	valid_0's binary_logloss: 0.120005
[51]	valid_0's binary_logloss: 0.118783
[52]	valid_0's binary_logloss: 0.117263
[53]	valid_0's binary_logloss: 0.115734
[54]	valid_0's binary_logloss: 0.114691
[55]	valid_0's binary_logloss: 0.113642
[56]	valid_0's binary_logloss: 0.112482
[57]	valid_0's binary_logloss: 0.111204
[58]	valid_0's binary_logloss: 0.1103
[59]	valid_0's binary_logloss: 0.109009
[60]	valid_0's binary_logloss: 0.10807
[61]	valid_0's binary_logloss: 0.107142
[62]	valid_0's binary_logloss: 0.106098
[63]	valid_0's binary_logloss: 0.105326
[64]	valid_0's binary_logloss: 0.104633
[65]	valid_0's binary_logloss: 0.103609
[66]	valid_0's binary_logloss: 0.102958
[67]	valid_0's binary_logloss: 0.102139
[68]	valid_0's binary_logloss: 0.101375
[69]	valid_0's binary_logloss: 0.100546
[70]	valid_0's binary_logloss: 0.0999275
[71]	valid_0's binary_logloss: 0.0991776
[72]	valid_0's binary_logloss: 0.0986847
[73]	valid_0's binary_logloss: 0.0981484
[74]	valid_0's binary_logloss: 0.0974286
[75]	valid_0's binary_logloss: 0.0968755
[76]	valid_0's binary_logloss: 0.0963083
[77]	valid_0's binary_logloss: 0.0956922
[78]	valid_0's binary_logloss: 0.0950353
[79]	valid_0's binary_logloss: 0.0945745
[80]	valid_0's binary_logloss: 0.0940401
[81]	valid_0's binary_logloss: 0.0934833
[82]	valid_0's binary_logloss: 0.0928967
[83]	valid_0's binary_logloss: 0.0924405
[84]	valid_0's binary_logloss: 0.09199
[85]	valid_0's binary_logloss: 0.0916153
[86]	valid_0's binary_logloss: 0.0910391
[87]	valid_0's binary_logloss: 0.0905191
[88]	valid_0's binary_logloss: 0.0901042
[89]	valid_0's binary_logloss: 0.0895922
[90]	valid_0's binary_logloss: 0.0892194
[91]	valid_0's binary_logloss: 0.0888885
[92]	valid_0's binary_logloss: 0.0884186
[93]	valid_0's binary_logloss: 0.0879577
[94]	valid_0's binary_logloss: 0.0875382
[95]	valid_0's binary_logloss: 0.0871463
[96]	valid_0's binary_logloss: 0.0867445
[97]	valid_0's binary_logloss: 0.0863659
[98]	valid_0's binary_logloss: 0.0860098
[99]	valid_0's binary_logloss: 0.0856546
[100]	valid_0's binary_logloss: 0.0852741
[101]	valid_0's binary_logloss: 0.0850073
[102]	valid_0's binary_logloss: 0.0847662
[103]	valid_0's binary_logloss: 0.0844319
[104]	valid_0's binary_logloss: 0.0840823
[105]	valid_0's binary_logloss: 0.0835963
[106]	valid_0's binary_logloss: 0.0833247
[107]	valid_0's binary_logloss: 0.0829848
[108]	valid_0's binary_logloss: 0.0826251
[109]	valid_0's binary_logloss: 0.0823877
[110]	valid_0's binary_logloss: 0.0821442
[111]	valid_0's binary_logloss: 0.0818241
[112]	valid_0's binary_logloss: 0.0815424
[113]	valid_0's binary_logloss: 0.0812755
[114]	valid_0's binary_logloss: 0.080986
[115]	valid_0's binary_logloss: 0.0807338
[116]	valid_0's binary_logloss: 0.0804929
[117]	valid_0's binary_logloss: 0.0802589
[118]	valid_0's binary_logloss: 0.0800056
[119]	valid_0's binary_logloss: 0.0797233
[120]	valid_0's binary_logloss: 0.0794643
[121]	valid_0's binary_logloss: 0.0792551
[122]	valid_0's binary_logloss: 0.0789708
[123]	valid_0's binary_logloss: 0.0787122
[124]	valid_0's binary_logloss: 0.0785134
[125]	valid_0's binary_logloss: 0.07827
[126]	valid_0's binary_logloss: 0.0779947
[127]	valid_0's binary_logloss: 0.0778066
[128]	valid_0's binary_logloss: 0.077584
[129]	valid_0's binary_logloss: 0.0774675
[130]	valid_0's binary_logloss: 0.0771915
[131]	valid_0's binary_logloss: 0.0769822
[132]	valid_0's binary_logloss: 0.0767988
[133]	valid_0's binary_logloss: 0.0766492
[134]	valid_0's binary_logloss: 0.0764188
[135]	valid_0's binary_logloss: 0.0762209
[136]	valid_0's binary_logloss: 0.0760033
[137]	valid_0's binary_logloss: 0.0757599
[138]	valid_0's binary_logloss: 0.0756864
[139]	valid_0's binary_logloss: 0.0754974
[140]	valid_0's binary_logloss: 0.0752996
[141]	valid_0's binary_logloss: 0.0751283
[142]	valid_0's binary_logloss: 0.0750078
[143]	valid_0's binary_logloss: 0.0747666
[144]	valid_0's binary_logloss: 0.0745585
[145]	valid_0's binary_logloss: 0.0743317
[146]	valid_0's binary_logloss: 0.0741758
[147]	valid_0's binary_logloss: 0.0741077
[148]	valid_0's binary_logloss: 0.0738999
[149]	valid_0's binary_logloss: 0.0736861
[150]	valid_0's binary_logloss: 0.0735205
[151]	valid_0's binary_logloss: 0.0733396
[152]	valid_0's binary_logloss: 0.0731914
[153]	valid_0's binary_logloss: 0.0730442
[154]	valid_0's binary_logloss: 0.0728772
[155]	valid_0's binary_logloss: 0.0727288
[156]	valid_0's binary_logloss: 0.0726771
[157]	valid_0's binary_logloss: 0.0725135
[158]	valid_0's binary_logloss: 0.0723339
[159]	valid_0's binary_logloss: 0.0722247
[160]	valid_0's binary_logloss: 0.0720909
[161]	valid_0's binary_logloss: 0.0720172
[162]	valid_0's binary_logloss: 0.0718424
[163]	valid_0's binary_logloss: 0.0716933
[164]	valid_0's binary_logloss: 0.0714829
[165]	valid_0's binary_logloss: 0.0713468
[166]	valid_0's binary_logloss: 0.0712655
[167]	valid_0's binary_logloss: 0.0710913
[168]	valid_0's binary_logloss: 0.0709153
[169]	valid_0's binary_logloss: 0.0707912
[170]	valid_0's binary_logloss: 0.0707128
[171]	valid_0's binary_logloss: 0.070553
[172]	valid_0's binary_logloss: 0.0704221
[173]	valid_0's binary_logloss: 0.0703101
[174]	valid_0's binary_logloss: 0.0701546
[175]	valid_0's binary_logloss: 0.0701039
[176]	valid_0's binary_logloss: 0.0700008
[177]	valid_0's binary_logloss: 0.0699379
[178]	valid_0's binary_logloss: 0.0697991
[179]	valid_0's binary_logloss: 0.0696425
[180]	valid_0's binary_logloss: 0.0695319
[181]	valid_0's binary_logloss: 0.0694309
[182]	valid_0's binary_logloss: 0.0692837
[183]	valid_0's binary_logloss: 0.0690933
[184]	valid_0's binary_logloss: 0.0689614
[185]	valid_0's binary_logloss: 0.0688567
[186]	valid_0's binary_logloss: 0.0687891
[187]	valid_0's binary_logloss: 0.0686006
[188]	valid_0's binary_logloss: 0.0684758
[189]	valid_0's binary_logloss: 0.0683404
[190]	valid_0's binary_logloss: 0.0682427
[191]	valid_0's binary_logloss: 0.0681462
[192]	valid_0's binary_logloss: 0.068034
[193]	valid_0's binary_logloss: 0.0679417
[194]	valid_0's binary_logloss: 0.0678413
[195]	valid_0's binary_logloss: 0.0677215
[196]	valid_0's binary_logloss: 0.0676833
[197]	valid_0's binary_logloss: 0.0675907
[198]	valid_0's binary_logloss: 0.0674685
[199]	valid_0's binary_logloss: 0.0673776
[200]	valid_0's binary_logloss: 0.0672742
[201]	valid_0's binary_logloss: 0.0671103
[202]	valid_0's binary_logloss: 0.0670115
[203]	valid_0's binary_logloss: 0.0669114
[204]	valid_0's binary_logloss: 0.0668705
[205]	valid_0's binary_logloss: 0.0667574
[206]	valid_0's binary_logloss: 0.0666759
[207]	valid_0's binary_logloss: 0.0666084
[208]	valid_0's binary_logloss: 0.0664906
[209]	valid_0's binary_logloss: 0.0663748
[210]	valid_0's binary_logloss: 0.0662894
[211]	valid_0's binary_logloss: 0.0662436
[212]	valid_0's binary_logloss: 0.0661744
[213]	valid_0's binary_logloss: 0.0660561
[214]	valid_0's binary_logloss: 0.0659504
[215]	valid_0's binary_logloss: 0.0658663
[216]	valid_0's binary_logloss: 0.0658364
[217]	valid_0's binary_logloss: 0.0657286
[218]	valid_0's binary_logloss: 0.0656405
[219]	valid_0's binary_logloss: 0.0655487
[220]	valid_0's binary_logloss: 0.0654496
[221]	valid_0's binary_logloss: 0.0653621
[222]	valid_0's binary_logloss: 0.0652493
[223]	valid_0's binary_logloss: 0.0651867
[224]	valid_0's binary_logloss: 0.065129
[225]	valid_0's binary_logloss: 0.0650446
[226]	valid_0's binary_logloss: 0.064975
[227]	valid_0's binary_logloss: 0.0648807
[228]	valid_0's binary_logloss: 0.0647709
[229]	valid_0's binary_logloss: 0.0646801
[230]	valid_0's binary_logloss: 0.0646186
[231]	valid_0's binary_logloss: 0.0645231
[232]	valid_0's binary_logloss: 0.0645143
[233]	valid_0's binary_logloss: 0.0644141
[234]	valid_0's binary_logloss: 0.0643453
[235]	valid_0's binary_logloss: 0.0642402
[236]	valid_0's binary_logloss: 0.0641704
[237]	valid_0's binary_logloss: 0.0640832
[238]	valid_0's binary_logloss: 0.0640505
[239]	valid_0's binary_logloss: 0.0639665
[240]	valid_0's binary_logloss: 0.0638984
[241]	valid_0's binary_logloss: 0.0638161
[242]	valid_0's binary_logloss: 0.0637463
[243]	valid_0's binary_logloss: 0.0636575
[244]	valid_0's binary_logloss: 0.063605
[245]	valid_0's binary_logloss: 0.0635646
[246]	valid_0's binary_logloss: 0.063497
[247]	valid_0's binary_logloss: 0.0634151
[248]	valid_0's binary_logloss: 0.0633779
[249]	valid_0's binary_logloss: 0.0632844
[250]	valid_0's binary_logloss: 0.0631716
[251]	valid_0's binary_logloss: 0.063117
[252]	valid_0's binary_logloss: 0.0630354
[253]	valid_0's binary_logloss: 0.0629588
[254]	valid_0's binary_logloss: 0.0628703
[255]	valid_0's binary_logloss: 0.0628064
[256]	valid_0's binary_logloss: 0.0628003
[257]	valid_0's binary_logloss: 0.0627367
[258]	valid_0's binary_logloss: 0.062649
[259]	valid_0's binary_logloss: 0.0625856
[260]	valid_0's binary_logloss: 0.06257
[261]	valid_0's binary_logloss: 0.0624865
[262]	valid_0's binary_logloss: 0.0624374
[263]	valid_0's binary_logloss: 0.0623489
[264]	valid_0's binary_logloss: 0.0622785
[265]	valid_0's binary_logloss: 0.0622126
[266]	valid_0's binary_logloss: 0.0621588
[267]	valid_0's binary_logloss: 0.0621111
[268]	valid_0's binary_logloss: 0.0620628
[269]	valid_0's binary_logloss: 0.0620083
[270]	valid_0's binary_logloss: 0.0619468
[271]	valid_0's binary_logloss: 0.0618578
[272]	valid_0's binary_logloss: 0.0618344
[273]	valid_0's binary_logloss: 0.0617567
[274]	valid_0's binary_logloss: 0.0617132
[275]	valid_0's binary_logloss: 0.0616306
[276]	valid_0's binary_logloss: 0.0615789
[277]	valid_0's binary_logloss: 0.0614833
[278]	valid_0's binary_logloss: 0.0614248
[279]	valid_0's binary_logloss: 0.0613725
[280]	valid_0's binary_logloss: 0.0613517
[281]	valid_0's binary_logloss: 0.0612958
[282]	valid_0's binary_logloss: 0.0612103
[283]	valid_0's binary_logloss: 0.0611771
[284]	valid_0's binary_logloss: 0.0611089
[285]	valid_0's binary_logloss: 0.0610613
[286]	valid_0's binary_logloss: 0.0609978
[287]	valid_0's binary_logloss: 0.0609625
[288]	valid_0's binary_logloss: 0.0609233
[289]	valid_0's binary_logloss: 0.060838
[290]	valid_0's binary_logloss: 0.0608253
[291]	valid_0's binary_logloss: 0.0607709
[292]	valid_0's binary_logloss: 0.0606783
[293]	valid_0's binary_logloss: 0.060637
[294]	valid_0's binary_logloss: 0.0605907
[295]	valid_0's binary_logloss: 0.0605685
[296]	valid_0's binary_logloss: 0.0605155
[297]	valid_0's binary_logloss: 0.0604746
[298]	valid_0's binary_logloss: 0.0604244
[299]	valid_0's binary_logloss: 0.0603611
[300]	valid_0's binary_logloss: 0.0603368
[301]	valid_0's binary_logloss: 0.0602959
[302]	valid_0's binary_logloss: 0.0602478
[303]	valid_0's binary_logloss: 0.0602058
[304]	valid_0's binary_logloss: 0.0601754
[305]	valid_0's binary_logloss: 0.0601328
[306]	valid_0's binary_logloss: 0.0600666
[307]	valid_0's binary_logloss: 0.0600423
[308]	valid_0's binary_logloss: 0.060017
[309]	valid_0's binary_logloss: 0.0599614
[310]	valid_0's binary_logloss: 0.0599253
[311]	valid_0's binary_logloss: 0.0598719
[312]	valid_0's binary_logloss: 0.0598464
[313]	valid_0's binary_logloss: 0.0598043
[314]	valid_0's binary_logloss: 0.0597699
[315]	valid_0's binary_logloss: 0.0597155
[316]	valid_0's binary_logloss: 0.0596456
[317]	valid_0's binary_logloss: 0.0596524
[318]	valid_0's binary_logloss: 0.0596121
[319]	valid_0's binary_logloss: 0.0595585
[320]	valid_0's binary_logloss: 0.0594963
[321]	valid_0's binary_logloss: 0.0594591
[322]	valid_0's binary_logloss: 0.0594344
[323]	valid_0's binary_logloss: 0.0593907
[324]	valid_0's binary_logloss: 0.0593468
[325]	valid_0's binary_logloss: 0.059309
[326]	valid_0's binary_logloss: 0.0593121
[327]	valid_0's binary_logloss: 0.0592723
[328]	valid_0's binary_logloss: 0.0592267
[329]	valid_0's binary_logloss: 0.0591806
[330]	valid_0's binary_logloss: 0.0591328
[331]	valid_0's binary_logloss: 0.0590732
[332]	valid_0's binary_logloss: 0.0590485
[333]	valid_0's binary_logloss: 0.0590307
[334]	valid_0's binary_logloss: 0.0589906
[335]	valid_0's binary_logloss: 0.0589314
[336]	valid_0's binary_logloss: 0.0588877
[337]	valid_0's binary_logloss: 0.0588503
[338]	valid_0's binary_logloss: 0.0587959
[339]	valid_0's binary_logloss: 0.0587625
[340]	valid_0's binary_logloss: 0.0587459
[341]	valid_0's binary_logloss: 0.0587004
[342]	valid_0's binary_logloss: 0.0586567
[343]	valid_0's binary_logloss: 0.0586153
[344]	valid_0's binary_logloss: 0.0585885
[345]	valid_0's binary_logloss: 0.0585331
[346]	valid_0's binary_logloss: 0.0585138
[347]	valid_0's binary_logloss: 0.0584764
[348]	valid_0's binary_logloss: 0.0584596
[349]	valid_0's binary_logloss: 0.0584173
[350]	valid_0's binary_logloss: 0.0583881
[351]	valid_0's binary_logloss: 0.0583382
[352]	valid_0's binary_logloss: 0.0583118
[353]	valid_0's binary_logloss: 0.0583156
[354]	valid_0's binary_logloss: 0.0582339
[355]	valid_0's binary_logloss: 0.0582129
[356]	valid_0's binary_logloss: 0.0581772
[357]	valid_0's binary_logloss: 0.0581468
[358]	valid_0's binary_logloss: 0.0581178
[359]	valid_0's binary_logloss: 0.0581067
[360]	valid_0's binary_logloss: 0.0580829
[361]	valid_0's binary_logloss: 0.0580299
[362]	valid_0's binary_logloss: 0.0579977
[363]	valid_0's binary_logloss: 0.0579583
[364]	valid_0's binary_logloss: 0.0579309
[365]	valid_0's binary_logloss: 0.0578842
[366]	valid_0's binary_logloss: 0.0578822
[367]	valid_0's binary_logloss: 0.0578601
[368]	valid_0's binary_logloss: 0.0578348
[369]	valid_0's binary_logloss: 0.0577962
[370]	valid_0's binary_logloss: 0.0577586
[371]	valid_0's binary_logloss: 0.0577304
[372]	valid_0's binary_logloss: 0.0577132
[373]	valid_0's binary_logloss: 0.0576945
[374]	valid_0's binary_logloss: 0.0576504
[375]	valid_0's binary_logloss: 0.0576239
[376]	valid_0's binary_logloss: 0.0575996
[377]	valid_0's binary_logloss: 0.0575723
[378]	valid_0's binary_logloss: 0.0575656
[379]	valid_0's binary_logloss: 0.0574924
[380]	valid_0's binary_logloss: 0.0574552
[381]	valid_0's binary_logloss: 0.0573956
[382]	valid_0's binary_logloss: 0.0573897
[383]	valid_0's binary_logloss: 0.0573649
[384]	valid_0's binary_logloss: 0.0573384
[385]	valid_0's binary_logloss: 0.0573364
[386]	valid_0's binary_logloss: 0.0573489
[387]	valid_0's binary_logloss: 0.0572787
[388]	valid_0's binary_logloss: 0.0572477
[389]	valid_0's binary_logloss: 0.0572177
[390]	valid_0's binary_logloss: 0.0571912
[391]	valid_0's binary_logloss: 0.0571886
[392]	valid_0's binary_logloss: 0.057168
[393]	valid_0's binary_logloss: 0.0571317
[394]	valid_0's binary_logloss: 0.0571307
[395]	valid_0's binary_logloss: 0.0571146
[396]	valid_0's binary_logloss: 0.0571263
[397]	valid_0's binary_logloss: 0.0570602
[398]	valid_0's binary_logloss: 0.0570435
[399]	valid_0's binary_logloss: 0.057008
[400]	valid_0's binary_logloss: 0.0569803
[401]	valid_0's binary_logloss: 0.0569412
[402]	valid_0's binary_logloss: 0.0569403
[403]	valid_0's binary_logloss: 0.0569494
[404]	valid_0's binary_logloss: 0.0568831
[405]	valid_0's binary_logloss: 0.0568555
[406]	valid_0's binary_logloss: 0.0568268
[407]	valid_0's binary_logloss: 0.0567914
[408]	valid_0's binary_logloss: 0.056779
[409]	valid_0's binary_logloss: 0.0567667
[410]	valid_0's binary_logloss: 0.056724
[411]	valid_0's binary_logloss: 0.056702
[412]	valid_0's binary_logloss: 0.0566709
[413]	valid_0's binary_logloss: 0.0566661
[414]	valid_0's binary_logloss: 0.0566241
[415]	valid_0's binary_logloss: 0.0566163
[416]	valid_0's binary_logloss: 0.056595
[417]	valid_0's binary_logloss: 0.0565661
[418]	valid_0's binary_logloss: 0.056526
[419]	valid_0's binary_logloss: 0.0565258
[420]	valid_0's binary_logloss: 0.056501
[421]	valid_0's binary_logloss: 0.0564644
[422]	valid_0's binary_logloss: 0.0563995
[423]	valid_0's binary_logloss: 0.0563825
[424]	valid_0's binary_logloss: 0.0563601
[425]	valid_0's binary_logloss: 0.0563292
[426]	valid_0's binary_logloss: 0.0562902
[427]	valid_0's binary_logloss: 0.0562677
[428]	valid_0's binary_logloss: 0.0562625
[429]	valid_0's binary_logloss: 0.0562496
[430]	valid_0's binary_logloss: 0.0562555
[431]	valid_0's binary_logloss: 0.0562636
[432]	valid_0's binary_logloss: 0.0562197
[433]	valid_0's binary_logloss: 0.0562089
[434]	valid_0's binary_logloss: 0.0561869
[435]	valid_0's binary_logloss: 0.0561508
[436]	valid_0's binary_logloss: 0.0561506
[437]	valid_0's binary_logloss: 0.0561155
[438]	valid_0's binary_logloss: 0.056104
[439]	valid_0's binary_logloss: 0.0560446
[440]	valid_0's binary_logloss: 0.0560401
[441]	valid_0's binary_logloss: 0.0560301
[442]	valid_0's binary_logloss: 0.0560363
[443]	valid_0's binary_logloss: 0.0560354
[444]	valid_0's binary_logloss: 0.055983
[445]	valid_0's binary_logloss: 0.0559646
[446]	valid_0's binary_logloss: 0.0559492
[447]	valid_0's binary_logloss: 0.0559004
[448]	valid_0's binary_logloss: 0.0558714
[449]	valid_0's binary_logloss: 0.0558716
[450]	valid_0's binary_logloss: 0.0558325
[451]	valid_0's binary_logloss: 0.0558142
[452]	valid_0's binary_logloss: 0.0557782
[453]	valid_0's binary_logloss: 0.0557426
[454]	valid_0's binary_logloss: 0.0557198
[455]	valid_0's binary_logloss: 0.0557224
[456]	valid_0's binary_logloss: 0.0557066
[457]	valid_0's binary_logloss: 0.0557034
[458]	valid_0's binary_logloss: 0.0556537
[459]	valid_0's binary_logloss: 0.0556327
[460]	valid_0's binary_logloss: 0.055632
[461]	valid_0's binary_logloss: 0.0556481
[462]	valid_0's binary_logloss: 0.0556126
[463]	valid_0's binary_logloss: 0.0555978
[464]	valid_0's binary_logloss: 0.0555742
[465]	valid_0's binary_logloss: 0.0555469
[466]	valid_0's binary_logloss: 0.0555206
[467]	valid_0's binary_logloss: 0.0554938
[468]	valid_0's binary_logloss: 0.0554924
[469]	valid_0's binary_logloss: 0.0554331
[470]	valid_0's binary_logloss: 0.0554045
[471]	valid_0's binary_logloss: 0.0554041
[472]	valid_0's binary_logloss: 0.0554202
[473]	valid_0's binary_logloss: 0.0554229
[474]	valid_0's binary_logloss: 0.0553967
[475]	valid_0's binary_logloss: 0.0553829
[476]	valid_0's binary_logloss: 0.0553558
[477]	valid_0's binary_logloss: 0.0553559
[478]	valid_0's binary_logloss: 0.0553238
[479]	valid_0's binary_logloss: 0.0552889
[480]	valid_0's binary_logloss: 0.0552634
[481]	valid_0's binary_logloss: 0.0552356
[482]	valid_0's binary_logloss: 0.0552336
[483]	valid_0's binary_logloss: 0.0551788
[484]	valid_0's binary_logloss: 0.05515
[485]	valid_0's binary_logloss: 0.0551434
[486]	valid_0's binary_logloss: 0.0551205
[487]	valid_0's binary_logloss: 0.0551079
[488]	valid_0's binary_logloss: 0.0551068
[489]	valid_0's binary_logloss: 0.0550737
[490]	valid_0's binary_logloss: 0.0550509
[491]	valid_0's binary_logloss: 0.0550373
[492]	valid_0's binary_logloss: 0.0550299
[493]	valid_0's binary_logloss: 0.0550036
[494]	valid_0's binary_logloss: 0.0550027
[495]	valid_0's binary_logloss: 0.0549897
[496]	valid_0's binary_logloss: 0.0549615
[497]	valid_0's binary_logloss: 0.0549535
[498]	valid_0's binary_logloss: 0.0549424
[499]	valid_0's binary_logloss: 0.0549162
[500]	valid_0's binary_logloss: 0.054913
[501]	valid_0's binary_logloss: 0.0548948
[502]	valid_0's binary_logloss: 0.0548906
[503]	valid_0's binary_logloss: 0.0548637
[504]	valid_0's binary_logloss: 0.0548342
[505]	valid_0's binary_logloss: 0.0548283
[506]	valid_0's binary_logloss: 0.0548192
[507]	valid_0's binary_logloss: 0.0547935
[508]	valid_0's binary_logloss: 0.0547911
[509]	valid_0's binary_logloss: 0.0547779
[510]	valid_0's binary_logloss: 0.0547504
[511]	valid_0's binary_logloss: 0.0547266
[512]	valid_0's binary_logloss: 0.054715
[513]	valid_0's binary_logloss: 0.0547077
[514]	valid_0's binary_logloss: 0.0547097
[515]	valid_0's binary_logloss: 0.0546833
[516]	valid_0's binary_logloss: 0.0546638
[517]	valid_0's binary_logloss: 0.0546456
[518]	valid_0's binary_logloss: 0.0546213
[519]	valid_0's binary_logloss: 0.0546268
[520]	valid_0's binary_logloss: 0.0546189
[521]	valid_0's binary_logloss: 0.0546077
[522]	valid_0's binary_logloss: 0.0545981
[523]	valid_0's binary_logloss: 0.0545505
[524]	valid_0's binary_logloss: 0.0545405
[525]	valid_0's binary_logloss: 0.054508
[526]	valid_0's binary_logloss: 0.0545099
[527]	valid_0's binary_logloss: 0.0545023
[528]	valid_0's binary_logloss: 0.0544841
[529]	valid_0's binary_logloss: 0.0544811
[530]	valid_0's binary_logloss: 0.0544772
[531]	valid_0's binary_logloss: 0.0544526
[532]	valid_0's binary_logloss: 0.0544532
[533]	valid_0's binary_logloss: 0.0544501
[534]	valid_0's binary_logloss: 0.0544496
[535]	valid_0's binary_logloss: 0.0544392
[536]	valid_0's binary_logloss: 0.0544152
[537]	valid_0's binary_logloss: 0.0544014
[538]	valid_0's binary_logloss: 0.0543829
[539]	valid_0's binary_logloss: 0.0543729
[540]	valid_0's binary_logloss: 0.0543523
[541]	valid_0's binary_logloss: 0.0543582
[542]	valid_0's binary_logloss: 0.0543412
[543]	valid_0's binary_logloss: 0.0543252
[544]	valid_0's binary_logloss: 0.0543083
[545]	valid_0's binary_logloss: 0.054285
[546]	valid_0's binary_logloss: 0.054276
[547]	valid_0's binary_logloss: 0.0542553
[548]	valid_0's binary_logloss: 0.0542578
[549]	valid_0's binary_logloss: 0.0542383
[550]	valid_0's binary_logloss: 0.0542374
[551]	valid_0's binary_logloss: 0.0542261
[552]	valid_0's binary_logloss: 0.0542042
[553]	valid_0's binary_logloss: 0.054195
[554]	valid_0's binary_logloss: 0.054185
[555]	valid_0's binary_logloss: 0.0541732
[556]	valid_0's binary_logloss: 0.0541678
[557]	valid_0's binary_logloss: 0.0541673
[558]	valid_0's binary_logloss: 0.0541791
[559]	valid_0's binary_logloss: 0.0541626
[560]	valid_0's binary_logloss: 0.0541394
[561]	valid_0's binary_logloss: 0.0541408
[562]	valid_0's binary_logloss: 0.0541289
[563]	valid_0's binary_logloss: 0.0541251
[564]	valid_0's binary_logloss: 0.054105
[565]	valid_0's binary_logloss: 0.0541064
[566]	valid_0's binary_logloss: 0.0540849
[567]	valid_0's binary_logloss: 0.0540863
[568]	valid_0's binary_logloss: 0.0540828
[569]	valid_0's binary_logloss: 0.054067
[570]	valid_0's binary_logloss: 0.0540602
[571]	valid_0's binary_logloss: 0.0540558
[572]	valid_0's binary_logloss: 0.0540349
[573]	valid_0's binary_logloss: 0.0540422
[574]	valid_0's binary_logloss: 0.0540348
[575]	valid_0's binary_logloss: 0.0540302
[576]	valid_0's binary_logloss: 0.054024
[577]	valid_0's binary_logloss: 0.0540182
[578]	valid_0's binary_logloss: 0.0539966
[579]	valid_0's binary_logloss: 0.0539953
[580]	valid_0's binary_logloss: 0.0539859
[581]	valid_0's binary_logloss: 0.053976
[582]	valid_0's binary_logloss: 0.0539658
[583]	valid_0's binary_logloss: 0.05395
[584]	valid_0's binary_logloss: 0.0539403
[585]	valid_0's binary_logloss: 0.0539409
[586]	valid_0's binary_logloss: 0.0539444
[587]	valid_0's binary_logloss: 0.0539415
[588]	valid_0's binary_logloss: 0.053933
[589]	valid_0's binary_logloss: 0.0539404
[590]	valid_0's binary_logloss: 0.0539211
[591]	valid_0's binary_logloss: 0.0539086
[592]	valid_0's binary_logloss: 0.0539015
[593]	valid_0's binary_logloss: 0.0539115
[594]	valid_0's binary_logloss: 0.0538906
[595]	valid_0's binary_logloss: 0.0538794
[596]	valid_0's binary_logloss: 0.0538634
[597]	valid_0's binary_logloss: 0.0538514
[598]	valid_0's binary_logloss: 0.0538205
[599]	valid_0's binary_logloss: 0.0538031
[600]	valid_0's binary_logloss: 0.0538083
[601]	valid_0's binary_logloss: 0.0538061
[602]	valid_0's binary_logloss: 0.0538152
[603]	valid_0's binary_logloss: 0.0538051
[604]	valid_0's binary_logloss: 0.0537948
[605]	valid_0's binary_logloss: 0.0537921
[606]	valid_0's binary_logloss: 0.0537745
[607]	valid_0's binary_logloss: 0.0537744
[608]	valid_0's binary_logloss: 0.0537736
[609]	valid_0's binary_logloss: 0.0537602
[610]	valid_0's binary_logloss: 0.0537455
[611]	valid_0's binary_logloss: 0.0537289
[612]	valid_0's binary_logloss: 0.0537299
[613]	valid_0's binary_logloss: 0.0537208
[614]	valid_0's binary_logloss: 0.0537223
[615]	valid_0's binary_logloss: 0.0537173
[616]	valid_0's binary_logloss: 0.0537172
[617]	valid_0's binary_logloss: 0.0537152
[618]	valid_0's binary_logloss: 0.0537174
[619]	valid_0's binary_logloss: 0.053717
[620]	valid_0's binary_logloss: 0.0537151
[621]	valid_0's binary_logloss: 0.0537174
[622]	valid_0's binary_logloss: 0.0537213
[623]	valid_0's binary_logloss: 0.0537127
[624]	valid_0's binary_logloss: 0.053723
[625]	valid_0's binary_logloss: 0.0537177
[626]	valid_0's binary_logloss: 0.0537174
[627]	valid_0's binary_logloss: 0.053709
[628]	valid_0's binary_logloss: 0.0537078
[629]	valid_0's binary_logloss: 0.0537096
[630]	valid_0's binary_logloss: 0.0537093
[631]	valid_0's binary_logloss: 0.0537011
[632]	valid_0's binary_logloss: 0.0536926
[633]	valid_0's binary_logloss: 0.0536848
[634]	valid_0's binary_logloss: 0.0536816
[635]	valid_0's binary_logloss: 0.0536751
[636]	valid_0's binary_logloss: 0.0536286
[637]	valid_0's binary_logloss: 0.0536129
[638]	valid_0's binary_logloss: 0.0535764
[639]	valid_0's binary_logloss: 0.0535744
[640]	valid_0's binary_logloss: 0.0535669
[641]	valid_0's binary_logloss: 0.0535682
[642]	valid_0's binary_logloss: 0.0535688
[643]	valid_0's binary_logloss: 0.053561
[644]	valid_0's binary_logloss: 0.0535619
[645]	valid_0's binary_logloss: 0.0535537
[646]	valid_0's binary_logloss: 0.0535573
[647]	valid_0's binary_logloss: 0.0535582
[648]	valid_0's binary_logloss: 0.0535585
[649]	valid_0's binary_logloss: 0.0535505
[650]	valid_0's binary_logloss: 0.0535407
[651]	valid_0's binary_logloss: 0.0535405
[652]	valid_0's binary_logloss: 0.0535459
[653]	valid_0's binary_logloss: 0.0535398
[654]	valid_0's binary_logloss: 0.0535399
[655]	valid_0's binary_logloss: 0.0535452
[656]	valid_0's binary_logloss: 0.0535516
[657]	valid_0's binary_logloss: 0.0535455
[658]	valid_0's binary_logloss: 0.0535378
[659]	valid_0's binary_logloss: 0.0535395
[660]	valid_0's binary_logloss: 0.0535448
[661]	valid_0's binary_logloss: 0.0535372
[662]	valid_0's binary_logloss: 0.0535408
[663]	valid_0's binary_logloss: 0.0535312
[664]	valid_0's binary_logloss: 0.0535321
[665]	valid_0's binary_logloss: 0.0535262
[666]	valid_0's binary_logloss: 0.0535267
[667]	valid_0's binary_logloss: 0.0535317
[668]	valid_0's binary_logloss: 0.0535258
[669]	valid_0's binary_logloss: 0.0535291
[670]	valid_0's binary_logloss: 0.0535228
[671]	valid_0's binary_logloss: 0.0535135
[672]	valid_0's binary_logloss: 0.0535215
[673]	valid_0's binary_logloss: 0.0535157
[674]	valid_0's binary_logloss: 0.0535129
[675]	valid_0's binary_logloss: 0.0535136
[676]	valid_0's binary_logloss: 0.0535141
[677]	valid_0's binary_logloss: 0.053508
[678]	valid_0's binary_logloss: 0.053514
[679]	valid_0's binary_logloss: 0.0535146
[680]	valid_0's binary_logloss: 0.0535162
[681]	valid_0's binary_logloss: 0.0535103
[682]	valid_0's binary_logloss: 0.0535172
[683]	valid_0's binary_logloss: 0.0535177
[684]	valid_0's binary_logloss: 0.0535119
[685]	valid_0's binary_logloss: 0.0535187
[686]	valid_0's binary_logloss: 0.0535192
[687]	valid_0's binary_logloss: 0.0535134
[688]	valid_0's binary_logloss: 0.0535194
[689]	valid_0's binary_logloss: 0.0535254
[690]	valid_0's binary_logloss: 0.0535259
[691]	valid_0's binary_logloss: 0.0535203
[692]	valid_0's binary_logloss: 0.0535125
[693]	valid_0's binary_logloss: 0.0535087
[694]	valid_0's binary_logloss: 0.0535093
[695]	valid_0's binary_logloss: 0.0535041
[696]	valid_0's binary_logloss: 0.0535045
[697]	valid_0's binary_logloss: 0.0535014
[698]	valid_0's binary_logloss: 0.0535045
[699]	valid_0's binary_logloss: 0.0535022
[700]	valid_0's binary_logloss: 0.0535082
[701]	valid_0's binary_logloss: 0.0535026
[702]	valid_0's binary_logloss: 0.0535035
[703]	valid_0's binary_logloss: 0.0535094
[704]	valid_0's binary_logloss: 0.053504
[705]	valid_0's binary_logloss: 0.0535048
[706]	valid_0's binary_logloss: 0.0535106
[707]	valid_0's binary_logloss: 0.0535055
[708]	valid_0's binary_logloss: 0.0535063
[709]	valid_0's binary_logloss: 0.0535014
[710]	valid_0's binary_logloss: 0.0535071
[711]	valid_0's binary_logloss: 0.0535041
[712]	valid_0's binary_logloss: 0.0535049
[713]	valid_0's binary_logloss: 0.0535037
[714]	valid_0's binary_logloss: 0.0534989
[715]	valid_0's binary_logloss: 0.053496
[716]	valid_0's binary_logloss: 0.0534979
[717]	valid_0's binary_logloss: 0.0534932
[718]	valid_0's binary_logloss: 0.0534933
[719]	valid_0's binary_logloss: 0.0534874
[720]	valid_0's binary_logloss: 0.0534918
[721]	valid_0's binary_logloss: 0.0534975
[722]	valid_0's binary_logloss: 0.0534928
[723]	valid_0's binary_logloss: 0.05349
[724]	valid_0's binary_logloss: 0.0534871
[725]	valid_0's binary_logloss: 0.0534825
[726]	valid_0's binary_logloss: 0.053478
[727]	valid_0's binary_logloss: 0.0534811
[728]	valid_0's binary_logloss: 0.0534767
[729]	valid_0's binary_logloss: 0.0534775
[730]	valid_0's binary_logloss: 0.0534789
[731]	valid_0's binary_logloss: 0.0534745
[732]	valid_0's binary_logloss: 0.0534703
[733]	valid_0's binary_logloss: 0.0534676
[734]	valid_0's binary_logloss: 0.0534707
[735]	valid_0's binary_logloss: 0.0534665
[736]	valid_0's binary_logloss: 0.0534638
[737]	valid_0's binary_logloss: 0.053464
[738]	valid_0's binary_logloss: 0.0534599
[739]	valid_0's binary_logloss: 0.0534526
[740]	valid_0's binary_logloss: 0.0534165
[741]	valid_0's binary_logloss: 0.0533874
[742]	valid_0's binary_logloss: 0.0533515
[743]	valid_0's binary_logloss: 0.0533383
[744]	valid_0's binary_logloss: 0.0533287
[745]	valid_0's binary_logloss: 0.0533194
[746]	valid_0's binary_logloss: 0.0533159
[747]	valid_0's binary_logloss: 0.0532989
[748]	valid_0's binary_logloss: 0.0532912
[749]	valid_0's binary_logloss: 0.0532938
[750]	valid_0's binary_logloss: 0.0532824
[751]	valid_0's binary_logloss: 0.0532792
[752]	valid_0's binary_logloss: 0.0532628
[753]	valid_0's binary_logloss: 0.0532589
[754]	valid_0's binary_logloss: 0.0532521
[755]	valid_0's binary_logloss: 0.0532418
[756]	valid_0's binary_logloss: 0.0532438
[757]	valid_0's binary_logloss: 0.0532414
[758]	valid_0's binary_logloss: 0.0532112
[759]	valid_0's binary_logloss: 0.053201
[760]	valid_0's binary_logloss: 0.0531766
[761]	valid_0's binary_logloss: 0.0531515
[762]	valid_0's binary_logloss: 0.0531295
[763]	valid_0's binary_logloss: 0.053117
[764]	valid_0's binary_logloss: 0.0531195
[765]	valid_0's binary_logloss: 0.0531035
[766]	valid_0's binary_logloss: 0.0530958
[767]	valid_0's binary_logloss: 0.0530983
[768]	valid_0's binary_logloss: 0.0530907
[769]	valid_0's binary_logloss: 0.0530812
[770]	valid_0's binary_logloss: 0.0530731
[771]	valid_0's binary_logloss: 0.0530651
[772]	valid_0's binary_logloss: 0.0530492
[773]	valid_0's binary_logloss: 0.0530414
[774]	valid_0's binary_logloss: 0.0530407
[775]	valid_0's binary_logloss: 0.0530416
[776]	valid_0's binary_logloss: 0.0530363
[777]	valid_0's binary_logloss: 0.0530373
[778]	valid_0's binary_logloss: 0.0530376
[779]	valid_0's binary_logloss: 0.0530366
[780]	valid_0's binary_logloss: 0.0530312
[781]	valid_0's binary_logloss: 0.0530303
[782]	valid_0's binary_logloss: 0.0530239
[783]	valid_0's binary_logloss: 0.0530219
[784]	valid_0's binary_logloss: 0.0530227
[785]	valid_0's binary_logloss: 0.0530207
[786]	valid_0's binary_logloss: 0.0530187
[787]	valid_0's binary_logloss: 0.0530185
[788]	valid_0's binary_logloss: 0.0530131
[789]	valid_0's binary_logloss: 0.0530125
[790]	valid_0's binary_logloss: 0.0530063
[791]	valid_0's binary_logloss: 0.0530044
[792]	valid_0's binary_logloss: 0.0530043
[793]	valid_0's binary_logloss: 0.0530038
[794]	valid_0's binary_logloss: 0.0529977
[795]	valid_0's binary_logloss: 0.0529927
[796]	valid_0's binary_logloss: 0.0529923
[797]	valid_0's binary_logloss: 0.0529874
[798]	valid_0's binary_logloss: 0.0529882
[799]	valid_0's binary_logloss: 0.0529901
[800]	valid_0's binary_logloss: 0.0529853
[801]	valid_0's binary_logloss: 0.0529807
[802]	valid_0's binary_logloss: 0.0529843
[803]	valid_0's binary_logloss: 0.0529797
[804]	valid_0's binary_logloss: 0.0529752
[805]	valid_0's binary_logloss: 0.0529761
[806]	valid_0's binary_logloss: 0.0529717
[807]	valid_0's binary_logloss: 0.0529725
[808]	valid_0's binary_logloss: 0.0529682
[809]	valid_0's binary_logloss: 0.0529691
[810]	valid_0's binary_logloss: 0.0529648
[811]	valid_0's binary_logloss: 0.0529607
[812]	valid_0's binary_logloss: 0.0529573
[813]	valid_0's binary_logloss: 0.0529532
[814]	valid_0's binary_logloss: 0.052954
[815]	valid_0's binary_logloss: 0.0529548
[816]	valid_0's binary_logloss: 0.0529557
[817]	valid_0's binary_logloss: 0.0529517
[818]	valid_0's binary_logloss: 0.0529499
[819]	valid_0's binary_logloss: 0.052949
[820]	valid_0's binary_logloss: 0.0529457
[821]	valid_0's binary_logloss: 0.0529419
[822]	valid_0's binary_logloss: 0.0529426
[823]	valid_0's binary_logloss: 0.0529464
[824]	valid_0's binary_logloss: 0.0529438
[825]	valid_0's binary_logloss: 0.0529468
[826]	valid_0's binary_logloss: 0.0529429
[827]	valid_0's binary_logloss: 0.0529392
[828]	valid_0's binary_logloss: 0.0529368
[829]	valid_0's binary_logloss: 0.0529397
[830]	valid_0's binary_logloss: 0.0529343
[831]	valid_0's binary_logloss: 0.0529308
[832]	valid_0's binary_logloss: 0.0529296
[833]	valid_0's binary_logloss: 0.0529303
[834]	valid_0's binary_logloss: 0.0529332
[835]	valid_0's binary_logloss: 0.0529308
[836]	valid_0's binary_logloss: 0.0529255
[837]	valid_0's binary_logloss: 0.052924
[838]	valid_0's binary_logloss: 0.0529194
[839]	valid_0's binary_logloss: 0.0529162
[840]	valid_0's binary_logloss: 0.0529152
[841]	valid_0's binary_logloss: 0.0529144
[842]	valid_0's binary_logloss: 0.052912
[843]	valid_0's binary_logloss: 0.0529122
[844]	valid_0's binary_logloss: 0.0529114
[845]	valid_0's binary_logloss: 0.0529133
[846]	valid_0's binary_logloss: 0.052911
[847]	valid_0's binary_logloss: 0.0529102
[848]	valid_0's binary_logloss: 0.0529095
[849]	valid_0's binary_logloss: 0.0529087
[850]	valid_0's binary_logloss: 0.0529079
[851]	valid_0's binary_logloss: 0.0529072
[852]	valid_0's binary_logloss: 0.0529062
[853]	valid_0's binary_logloss: 0.0529011
[854]	valid_0's binary_logloss: 0.0528952
[855]	valid_0's binary_logloss: 0.0528955
[856]	valid_0's binary_logloss: 0.0528945
[857]	valid_0's binary_logloss: 0.0528895
[858]	valid_0's binary_logloss: 0.0528905
[859]	valid_0's binary_logloss: 0.0528897
[860]	valid_0's binary_logloss: 0.0528964
[861]	valid_0's binary_logloss: 0.0528958
[862]	valid_0's binary_logloss: 0.0528952
[863]	valid_0's binary_logloss: 0.0528945
[864]	valid_0's binary_logloss: 0.0528937
[865]	valid_0's binary_logloss: 0.0528932
[866]	valid_0's binary_logloss: 0.0528925
[867]	valid_0's binary_logloss: 0.0528903
[868]	valid_0's binary_logloss: 0.0528896
[869]	valid_0's binary_logloss: 0.0528892
[870]	valid_0's binary_logloss: 0.0528888
[871]	valid_0's binary_logloss: 0.0528885
[872]	valid_0's binary_logloss: 0.0528876
[873]	valid_0's binary_logloss: 0.0528872
[874]	valid_0's binary_logloss: 0.0528868
[875]	valid_0's binary_logloss: 0.0528863
[876]	valid_0's binary_logloss: 0.052886
[877]	valid_0's binary_logloss: 0.052881
[878]	valid_0's binary_logloss: 0.0528812
[879]	valid_0's binary_logloss: 0.0528808
[880]	valid_0's binary_logloss: 0.0528803
[881]	valid_0's binary_logloss: 0.0528798
[882]	valid_0's binary_logloss: 0.0528794
[883]	valid_0's binary_logloss: 0.0528789
[884]	valid_0's binary_logloss: 0.0528785
[885]	valid_0's binary_logloss: 0.0528777
[886]	valid_0's binary_logloss: 0.0528768
[887]	valid_0's binary_logloss: 0.052872
[888]	valid_0's binary_logloss: 0.0528722
[889]	valid_0's binary_logloss: 0.0528717
[890]	valid_0's binary_logloss: 0.0528713
[891]	valid_0's binary_logloss: 0.0528708
[892]	valid_0's binary_logloss: 0.0528703
[893]	valid_0's binary_logloss: 0.0528699
[894]	valid_0's binary_logloss: 0.0528695
[895]	valid_0's binary_logloss: 0.0528688
[896]	valid_0's binary_logloss: 0.0528704
[897]	valid_0's binary_logloss: 0.0528579
[898]	valid_0's binary_logloss: 0.0528646
[899]	valid_0's binary_logloss: 0.0528641
[900]	valid_0's binary_logloss: 0.0528637
[901]	valid_0's binary_logloss: 0.0528632
[902]	valid_0's binary_logloss: 0.0528628
[903]	valid_0's binary_logloss: 0.0528624
[904]	valid_0's binary_logloss: 0.0528619
[905]	valid_0's binary_logloss: 0.0528615
[906]	valid_0's binary_logloss: 0.0528611
[907]	valid_0's binary_logloss: 0.0528608
[908]	valid_0's binary_logloss: 0.0528604
[909]	valid_0's binary_logloss: 0.05286
[910]	valid_0's binary_logloss: 0.0528597
[911]	valid_0's binary_logloss: 0.0528593
[912]	valid_0's binary_logloss: 0.052859
[913]	valid_0's binary_logloss: 0.0528586
[914]	valid_0's binary_logloss: 0.0528583
[915]	valid_0's binary_logloss: 0.052858
[916]	valid_0's binary_logloss: 0.0528577
[917]	valid_0's binary_logloss: 0.0528574
[918]	valid_0's binary_logloss: 0.0528551
[919]	valid_0's binary_logloss: 0.0528546
[920]	valid_0's binary_logloss: 0.0528541
[921]	valid_0's binary_logloss: 0.0528532
[922]	valid_0's binary_logloss: 0.0528541
[923]	valid_0's binary_logloss: 0.0528537
[924]	valid_0's binary_logloss: 0.0528533
[925]	valid_0's binary_logloss: 0.0528529
[926]	valid_0's binary_logloss: 0.0528526
[927]	valid_0's binary_logloss: 0.0528522
[928]	valid_0's binary_logloss: 0.0528531
[929]	valid_0's binary_logloss: 0.0528527
[930]	valid_0's binary_logloss: 0.0528524
[931]	valid_0's binary_logloss: 0.052852
[932]	valid_0's binary_logloss: 0.0528517
[933]	valid_0's binary_logloss: 0.0528514
[934]	valid_0's binary_logloss: 0.0528511
[935]	valid_0's binary_logloss: 0.0528507
[936]	valid_0's binary_logloss: 0.0528504
[937]	valid_0's binary_logloss: 0.0528501
[938]	valid_0's binary_logloss: 0.0528498
[939]	valid_0's binary_logloss: 0.052849
[940]	valid_0's binary_logloss: 0.0528411
[941]	valid_0's binary_logloss: 0.0528414
[942]	valid_0's binary_logloss: 0.0528427
[943]	valid_0's binary_logloss: 0.0528405
[944]	valid_0's binary_logloss: 0.0528399
[945]	valid_0's binary_logloss: 0.0528394
[946]	valid_0's binary_logloss: 0.0528389
[947]	valid_0's binary_logloss: 0.0528384
[948]	valid_0's binary_logloss: 0.0528392
[949]	valid_0's binary_logloss: 0.0528388
[950]	valid_0's binary_logloss: 0.0528383
[951]	valid_0's binary_logloss: 0.0528379
[952]	valid_0's binary_logloss: 0.0528374
[953]	valid_0's binary_logloss: 0.052837
[954]	valid_0's binary_logloss: 0.0528366
[955]	valid_0's binary_logloss: 0.0528374
[956]	valid_0's binary_logloss: 0.052837
[957]	valid_0's binary_logloss: 0.0528367
[958]	valid_0's binary_logloss: 0.0528363
[959]	valid_0's binary_logloss: 0.0528359
[960]	valid_0's binary_logloss: 0.0528356
[961]	valid_0's binary_logloss: 0.0528352
[962]	valid_0's binary_logloss: 0.0528349
[963]	valid_0's binary_logloss: 0.052834
[964]	valid_0's binary_logloss: 0.0528329
[965]	valid_0's binary_logloss: 0.0528338
[966]	valid_0's binary_logloss: 0.0528333
[967]	valid_0's binary_logloss: 0.0528328
[968]	valid_0's binary_logloss: 0.0528324
[969]	valid_0's binary_logloss: 0.052832
[970]	valid_0's binary_logloss: 0.0528316
[971]	valid_0's binary_logloss: 0.0528307
[972]	valid_0's binary_logloss: 0.0528315
[973]	valid_0's binary_logloss: 0.0528312
[974]	valid_0's binary_logloss: 0.0528308
[975]	valid_0's binary_logloss: 0.0528305
[976]	valid_0's binary_logloss: 0.0528301
[977]	valid_0's binary_logloss: 0.0528298
[978]	valid_0's binary_logloss: 0.0528289
[979]	valid_0's binary_logloss: 0.0528279
[980]	valid_0's binary_logloss: 0.0528287
[981]	valid_0's binary_logloss: 0.0528283
[982]	valid_0's binary_logloss: 0.0528278
[983]	valid_0's binary_logloss: 0.0528274
[984]	valid_0's binary_logloss: 0.0528269
[985]	valid_0's binary_logloss: 0.0528277
[986]	valid_0's binary_logloss: 0.0528273
[987]	valid_0's binary_logloss: 0.0528269
[988]	valid_0's binary_logloss: 0.0528265
[989]	valid_0's binary_logloss: 0.0528261
[990]	valid_0's binary_logloss: 0.0528257
[991]	valid_0's binary_logloss: 0.0528253
[992]	valid_0's binary_logloss: 0.0528249
[993]	valid_0's binary_logloss: 0.052824
[994]	valid_0's binary_logloss: 0.0528248
[995]	valid_0's binary_logloss: 0.0528238
[996]	valid_0's binary_logloss: 0.0528234
[997]	valid_0's binary_logloss: 0.0528229
[998]	valid_0's binary_logloss: 0.0528224
[999]	valid_0's binary_logloss: 0.052822
[1000]	valid_0's binary_logloss: 0.052823
Did not meet early stopping. Best iteration is:
[999]	valid_0's binary_logloss: 0.052822
LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               metric='binary_logloss', min_child_samples=20,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,
               n_jobs=-1, num_iterations=1000, num_leaves=31,
               objective='binary', random_state=None, reg_alpha=5,
               reg_lambda=120, silent=True, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
gbm_train_probs = gbm_clf.predict_proba(x_train)
gbm_test_probs = gbm_clf.predict_proba(x_test)
gbm_train_prob_cols = ['V', 'proba']

gbm_preds_train = pd.DataFrame(gbm_train_probs, columns=gbm_train_prob_cols)
gbm_preds_test = pd.DataFrame(gbm_test_probs, columns=gbm_train_prob_cols)
fpr, tpr, thresholds = metrics.roc_curve(y_train, gbm_preds_train['proba'])
metrics.auc(fpr, tpr)
0.9997666481990324
fpr, tpr, thresholds = metrics.roc_curve(y_test, gbm_preds_test['proba'])
metrics.auc(fpr, tpr)
0.9981828994317431
figure(num=None, figsize = (20,20))
gbm_split_imp_plot = gbm.plot_importance(gbm_clf)
fig = gbm_split_imp_plot.figure
fig.set_size_inches(20,20)
<Figure size 1440x1440 with 0 Axes>

import shap
gbm_shap_explainer = shap.TreeExplainer(bst)
gbm_shap_vals_train = gbm_shap_explainer.shap_values(x_train)
gbm_shap_vals_train.dtype
dtype('float32')
gbm_shap_vals_test = gbm_shap_explainer.shap_values(x_test)
shap.initjs()
shap.force_plot(gbm_shap_explainer.expected_value, gbm_shap_vals_train[0,:], x_train.iloc[0,:])

shap.initjs()
shap.dependence_plot("Overall", gbm_shap_vals_test, x_test)


shap.summary_plot(gbm_shap_vals_train, x_train)

shap.summary_plot(gbm_shap_vals_test, x_test)

gbm_var_interactions = shap.TreeExplainer(bst).shap_interaction_values(x_test)
shap.dependence_plot(("Age","Potential"),
                     gbm_var_interactions, x_test,
                     display_features = x_test)

 
